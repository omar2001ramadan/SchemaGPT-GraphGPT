{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create a new directory to save all files\n",
    "output_dir = 'output_files'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define global threshold variable\n",
    "threshold = 0.8\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by lowercasing and removing punctuation.\"\"\"\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def get_embeddings(text):\n",
    "    \"\"\"Get BERT embeddings for the given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Last hidden state (embedding)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "def save_tokenized_texts_combined(tokenized_texts, file_label):\n",
    "    \"\"\"Save combined tokenized texts to a file.\"\"\"\n",
    "    with open(os.path.join(output_dir, f\"combined_tokenized_output_{file_label}.txt\"), \"w\") as f:\n",
    "        for tokenized_text in tokenized_texts:\n",
    "            f.write(\" \".join(tokenized_text) + \"\\n\")\n",
    "\n",
    "def save_embeddings_combined(embeddings, file_label):\n",
    "    \"\"\"Save combined embeddings as .npy file.\"\"\"\n",
    "    combined_embeddings = np.vstack(embeddings)\n",
    "    np.save(os.path.join(output_dir, f\"combined_embeddings_{file_label}.npy\"), combined_embeddings)\n",
    "\n",
    "def json_to_text(data, file_label):\n",
    "    \"\"\"Convert JSON entries to text for analysis and remove 'id' key. Add a 'file' key and determine node_or_edge.\"\"\"\n",
    "    texts = []\n",
    "    embeddings = []\n",
    "    tokenized_texts = []\n",
    "    json_objects = {}\n",
    "    original_ids = {}\n",
    "    labels = []\n",
    "    for i, entry in enumerate(data):\n",
    "        original_id = entry.pop('id', None)  # Remove 'id' key and store it\n",
    "        if original_id:\n",
    "            original_ids[original_id] = entry  # Map original 'id' to entry\n",
    "\n",
    "        entry['file'] = file_label  # Add 'file' key\n",
    "        entry_items = list(entry.items())  # Get all key-value pairs\n",
    "        first_key_value = entry_items[0] if len(entry_items) > 0 else (\"None\", \"None\")\n",
    "        second_key_value = entry_items[1] if len(entry_items) > 1 else (\"None\", \"None\")\n",
    "        labels.append(f\"{first_key_value[0]}: {first_key_value[1]} ({file_label}), {second_key_value[0]}: {second_key_value[1]} ({file_label})\")\n",
    "        text = \". \".join(f\"{key}: {value}\" if not isinstance(value, list) else f\"{key}: \" + \", \".join(map(str, value)) for key, value in entry.items())\n",
    "        text = preprocess_text(text)\n",
    "        texts.append(text)\n",
    "\n",
    "        # Tokenization\n",
    "        tokenized_text = tokenizer.tokenize(text)\n",
    "        print(f\"Tokenized {file_label} object {i + 1}: {tokenized_text}\")\n",
    "        tokenized_texts.append(tokenized_text)\n",
    "\n",
    "        # Get and save embeddings\n",
    "        embedding = get_embeddings(text)\n",
    "        print(f\"Embedding shape for {file_label} object {i + 1}: {embedding.shape}\")\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "        # Determine node_or_edge\n",
    "        if 'source_ref' in entry and 'target_ref' in entry:\n",
    "            entry['node_or_edge'] = \"edge\"\n",
    "        else:\n",
    "            entry['node_or_edge'] = \"node\"\n",
    "        \n",
    "        json_objects[text] = {'entry': entry, 'first_key_value': first_key_value, 'second_key_value': second_key_value}\n",
    "    return texts, embeddings, tokenized_texts, json_objects, original_ids, labels\n",
    "\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"Load JSON data from a file with detailed error handling.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.read()\n",
    "            return json.loads(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Failed to decode JSON from file - {file_path}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        print(f\"Faulty JSON content:\\n{data}\")\n",
    "        return None\n",
    "\n",
    "def assign_encode_ids(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, original_ids1, original_ids2, threshold):\n",
    "    \"\"\"Assign encodeID to similar JSON objects in two datasets, ensuring cross-file matching and update source_ref/target_ref.\"\"\"\n",
    "    combined_texts = texts1 + texts2\n",
    "    combined_embeddings = np.vstack(embeddings1 + embeddings2)\n",
    "    combined_json_objects = {**json_objects1, **json_objects2}\n",
    "\n",
    "    # Ensure all objects have 'first_key_value', 'second_key_value', and 'entry'\n",
    "    for text in combined_texts:\n",
    "        if text not in combined_json_objects:\n",
    "            continue\n",
    "        if 'first_key_value' not in combined_json_objects[text] or 'second_key_value' not in combined_json_objects[text] or 'entry' not in combined_json_objects[text]:\n",
    "            print(f\"Missing 'first_key_value', 'second_key_value' or 'entry' in combined_json_objects for text: {text}\")\n",
    "            print(combined_json_objects[text])\n",
    "            continue\n",
    "\n",
    "    # Compute similarity matrix for combined texts\n",
    "    similarity_matrix = cosine_similarity(combined_embeddings)\n",
    "    encode_id = 1\n",
    "    unmatched_encode_id = 10000\n",
    "    encode_id_mapping = {}\n",
    "    assigned_encode_ids = set()\n",
    "    max_similarity = {}\n",
    "\n",
    "    # Create comparison matrix and identify matches\n",
    "    num_texts = len(combined_texts)\n",
    "    comparison_matrix = np.zeros((num_texts, num_texts))\n",
    "    for i in range(num_texts):\n",
    "        for j in range(num_texts):\n",
    "            if i != j:\n",
    "                similarity = similarity_matrix[i, j]\n",
    "                comparison_matrix[i, j] = similarity\n",
    "                if similarity > threshold and combined_json_objects[combined_texts[i]]['entry']['file'] != combined_json_objects[combined_texts[j]]['entry']['file']:\n",
    "                    if combined_texts[i] not in assigned_encode_ids or similarity > max_similarity.get(combined_texts[i], 0):\n",
    "                        encode_id_mapping[combined_texts[i]] = encode_id\n",
    "                        encode_id_mapping[combined_texts[j]] = encode_id\n",
    "                        assigned_encode_ids.add(combined_texts[i])\n",
    "                        assigned_encode_ids.add(combined_texts[j])\n",
    "                        max_similarity[combined_texts[i]] = similarity\n",
    "                        max_similarity[combined_texts[j]] = similarity\n",
    "                        encode_id += 1\n",
    "\n",
    "    for text in combined_texts:\n",
    "        if text not in encode_id_mapping:\n",
    "            encode_id_mapping[text] = unmatched_encode_id\n",
    "            unmatched_encode_id += 1\n",
    "\n",
    "    for text, obj in combined_json_objects.items():\n",
    "        if text in encode_id_mapping:\n",
    "            obj['entry']['encodeID'] = encode_id_mapping[text]\n",
    "\n",
    "    json_objects1 = {text: obj['entry'] for text, obj in combined_json_objects.items() if obj['entry']['file'] == 'json1'}\n",
    "    json_objects2 = {text: obj['entry'] for text, obj in combined_json_objects.items() if obj['entry']['file'] == 'json2'}\n",
    "\n",
    "    # Create a dictionary to map original IDs to encodeIDs\n",
    "    id_to_encodeID = {original_id: obj['encodeID'] for original_id, obj in original_ids1.items()}\n",
    "    id_to_encodeID.update({original_id: obj['encodeID'] for original_id, obj in original_ids2.items()})\n",
    "\n",
    "    # Update source_ref and target_ref in relationship objects\n",
    "    for obj in json_objects1.values():\n",
    "        if obj['node_or_edge'] == 'edge':\n",
    "            obj['source_ref'] = id_to_encodeID.get(obj['source_ref'], obj['source_ref'])\n",
    "            obj['target_ref'] = id_to_encodeID.get(obj['target_ref'], obj['target_ref'])\n",
    "\n",
    "    for obj in json_objects2.values():\n",
    "        if obj['node_or_edge'] == 'edge':\n",
    "            obj['source_ref'] = id_to_encodeID.get(obj['source_ref'], obj['source_ref'])\n",
    "            obj['target_ref'] = id_to_encodeID.get(obj['target_ref'], obj['target_ref'])\n",
    "\n",
    "    return json_objects1, json_objects2, comparison_matrix, combined_embeddings\n",
    "\n",
    "def identify_node_or_edge_and_add_key(json_objects):\n",
    "    \"\"\"Identify if an object is a node or an edge based on the presence of two values matching encodeIDs of other objects,\n",
    "    and add a new key-value pair indicating its type.\"\"\"\n",
    "    \n",
    "    encode_ids = {entry['encodeID'] for entry in json_objects.values()}\n",
    "    \n",
    "    for obj in json_objects.values():\n",
    "        values = set(obj.values())\n",
    "        if len(values.intersection(encode_ids)) >= 2:\n",
    "            obj['node_or_edge'] = \"edge\"\n",
    "        else:\n",
    "            obj['node_or_edge'] = \"node\"\n",
    "    \n",
    "    return json_objects\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    \"\"\"Save JSON data to a file.\"\"\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "\n",
    "def build_graph(json_objects, suffix, color):\n",
    "    \"\"\"Build a graph based on JSON objects.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    valid_nodes = set()\n",
    "\n",
    "    # Add nodes, ensuring unique IDs\n",
    "    for key, obj in json_objects.items():\n",
    "        if obj['node_or_edge'] != 'edge':  # Only add entities as nodes\n",
    "            if obj.get('name') != 'Unknown Name':  # Avoid adding unknown name nodes\n",
    "                unique_id = str(obj.get('encodeID')) + suffix\n",
    "                node_name = f\"{obj.get('name', 'Unknown Entity')} ({color})\"\n",
    "                G.add_node(unique_id, label=node_name, color=color,\n",
    "                           description=obj.get('description', 'No description provided'))\n",
    "                valid_nodes.add(unique_id)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Add edges, using unique IDs\n",
    "    unique_edges = set()\n",
    "    for obj in json_objects.values():\n",
    "        if obj['node_or_edge'] == 'edge':\n",
    "            source = str(obj.get('source_ref')) + suffix\n",
    "            target = str(obj.get('target_ref')) + suffix\n",
    "            edge = (source, target)\n",
    "            if source in valid_nodes and target in valid_nodes and source != target:  # Ensure both nodes are valid and not the same\n",
    "                if edge in unique_edges:\n",
    "                    G[source][target]['style'] = 'solid'\n",
    "                    G[source][target]['color'] = 'black'\n",
    "                else:\n",
    "                    G.add_edge(source, target, style='dotted', color=color, label=obj.get('relationship_type', 'similar-to'))\n",
    "                    unique_edges.add(edge)\n",
    "    return G\n",
    "\n",
    "def visualize_graph(G):\n",
    "    \"\"\"Visualize a networkx graph with enhanced label formatting for readability.\"\"\"\n",
    "    num_nodes = len(G.nodes)\n",
    "    figsize = get_optimal_figsize(num_nodes)\n",
    "    k = get_optimal_k(num_nodes)\n",
    "\n",
    "    plt.figure(figsize=figsize)  # Dynamically set figure size\n",
    "    pos = nx.spring_layout(G, k=k)  # Dynamically set layout parameter\n",
    "\n",
    "    # Draw nodes with their corresponding colors\n",
    "    node_colors = [data.get('color', 'grey') for node, data in G.nodes(data=True)]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=300)\n",
    "\n",
    "    # Draw edges with specified styles and colors\n",
    "    for (u, v, data) in G.edges(data=True):\n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos, edgelist=[(u, v)],\n",
    "            style=data.get('style', 'dotted' if data['color'] == 'gray' else 'solid'),\n",
    "            edge_color=data.get('color', 'black'),\n",
    "            width=2\n",
    "        )\n",
    "\n",
    "    # Draw edge labels\n",
    "    edge_labels = {(u, v): data.get('label', 'similar-to') for u, v, data in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "    # Draw labels for nodes\n",
    "    labels = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        label = f\"{data.get('label', 'Unknown')}\\n({data.get('type', 'Unknown')})\"\n",
    "        description = data.get('description', 'No description provided')\n",
    "        if description:\n",
    "            doc = nlp(description)\n",
    "            verbs = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "            label += f\"\\n{' '.join(verbs[:2])}\"\n",
    "        labels[node] = label\n",
    "\n",
    "    for node, label in labels.items():\n",
    "        x, y = pos[node]\n",
    "        plt.text(x, y, label, fontsize=9, ha='center', va='center',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='white', edgecolor='gray', alpha=0.6))\n",
    "\n",
    "    plt.title('Graph Visualization')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_optimal_figsize(num_nodes):\n",
    "    \"\"\"Determine an optimal figure size based on the number of nodes.\"\"\"\n",
    "    base_size = 10  # Base size for the figure\n",
    "    scale_factor = 0.5  # Scale factor to adjust size\n",
    "    return (base_size + num_nodes * scale_factor, base_size + num_nodes * scale_factor)\n",
    "\n",
    "def get_optimal_k(num_nodes):\n",
    "    \"\"\"Determine an optimal k value for spring_layout based on the number of nodes.\"\"\"\n",
    "    base_k = 0.5  # Base k value\n",
    "    scale_factor = 0.1  # Scale factor to adjust k\n",
    "    return base_k + num_nodes * scale_factor\n",
    "\n",
    "def calculate_distances(json_objects1, json_objects2):\n",
    "    \"\"\"Calculate node distance, key distance, value distance, and graph distance.\"\"\"\n",
    "    def jaccard_distance(set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return 1 - intersection / union if union != 0 else 1\n",
    "\n",
    "    def set_distance(set1, set2):\n",
    "        return abs(len(set1) - len(set2))\n",
    "\n",
    "    # Node distance\n",
    "    nodes1 = set(json_objects1.keys())\n",
    "    nodes2 = set(json_objects2.keys())\n",
    "    node_distance = set_distance(nodes1, nodes2)\n",
    "    node_jaccard = jaccard_distance(nodes1, nodes2)\n",
    "\n",
    "    # Key distance\n",
    "    keys1 = set(k for obj in json_objects1.values() for k in obj.keys())\n",
    "    keys2 = set(k for obj in json_objects2.values() for k in obj.keys())\n",
    "    key_distance = set_distance(keys1, keys2)\n",
    "    key_jaccard = jaccard_distance(keys1, keys2)\n",
    "\n",
    "    # Value distance\n",
    "    values1 = set(v for obj in json_objects1.values() for v in obj.values() if isinstance(v, str))\n",
    "    values2 = set(v for obj in json_objects2.values() for v in obj.values() if isinstance(v, str))\n",
    "    value_distance = set_distance(values1, values2)\n",
    "    value_jaccard = jaccard_distance(values1, values2)\n",
    "\n",
    "    # Graph distance (using node and edge comparison)\n",
    "    G1 = build_graph(json_objects1, '_blue', 'blue')\n",
    "    G2 = build_graph(json_objects2, '_red', 'red')\n",
    "    graph_distance = abs(G1.number_of_nodes() - G2.number_of_nodes()) + abs(G1.number_of_edges() - G2.number_of_edges())\n",
    "    graph_jaccard = jaccard_distance(set(G1.edges()), set(G2.edges()))\n",
    "\n",
    "    print(f\"Node Distance (Simple): {node_distance}\")\n",
    "    print(f\"Node Distance (Jaccard): {node_jaccard:.2f}\")\n",
    "    print(f\"Key Distance (Simple): {key_distance}\")\n",
    "    print(f\"Key Distance (Jaccard): {key_jaccard:.2f}\")\n",
    "    print(f\"Value Distance (Simple): {value_distance}\")\n",
    "    print(f\"Value Distance (Jaccard): {value_jaccard:.2f}\")\n",
    "    print(f\"Graph Distance (Simple): {graph_distance}\")\n",
    "    print(f\"Graph Distance (Jaccard): {graph_jaccard:.2f}\")\n",
    "\n",
    "def create_comparison_matrix(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, threshold):\n",
    "    \"\"\"Create a comparison matrix and identify matches.\"\"\"\n",
    "    combined_embeddings = np.vstack(embeddings1 + embeddings2)\n",
    "\n",
    "    # Compute similarity matrix for combined texts\n",
    "    similarity_matrix = cosine_similarity(combined_embeddings)\n",
    "\n",
    "    # Create comparison matrix for texts from json1 and json2 only\n",
    "    num_texts1 = len(texts1)\n",
    "    num_texts2 = len(texts2)\n",
    "    comparison_data = []\n",
    "    for i in range(num_texts1):\n",
    "        row = []\n",
    "        for j in range(num_texts2):\n",
    "            similarity = similarity_matrix[i, num_texts1 + j]\n",
    "            match = 1 if similarity > threshold else 0\n",
    "            row.append(match)\n",
    "        comparison_data.append(row)\n",
    "\n",
    "    # Create DataFrame for comparison matrix\n",
    "    labels1 = []\n",
    "    labels2 = []\n",
    "    label_to_text1 = {}\n",
    "    label_to_text2 = {}\n",
    "\n",
    "    for text, obj in json_objects1.items():\n",
    "        # Dynamically get the first and second key-value pairs\n",
    "        entry_items = list(obj.items())\n",
    "        first_key_value = entry_items[0] if len(entry_items) > 0 else (\"None\", \"None\")\n",
    "        second_key_value = entry_items[1] if len(entry_items) > 1 else (\"None\", \"None\")\n",
    "        label = f\"{first_key_value[0]}: {first_key_value[1]} ({obj['file']}), {second_key_value[0]}: {second_key_value[1]} ({obj['file']})\"\n",
    "        labels1.append(label)\n",
    "        label_to_text1[label] = text\n",
    "\n",
    "    for text, obj in json_objects2.items():\n",
    "        # Dynamically get the first and second key-value pairs\n",
    "        entry_items = list(obj.items())\n",
    "        first_key_value = entry_items[0] if len(entry_items) > 0 else (\"None\", \"None\")\n",
    "        second_key_value = entry_items[1] if len(entry_items) > 1 else (\"None\", \"None\")\n",
    "        label = f\"{first_key_value[0]}: {first_key_value[1]} ({obj['file']}), {second_key_value[0]}: {second_key_value[1]} ({obj['file']})\"\n",
    "        labels2.append(label)\n",
    "        label_to_text2[label] = text\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data, index=labels1, columns=labels2)\n",
    "\n",
    "    return comparison_df, similarity_matrix, label_to_text1, label_to_text2\n",
    "\n",
    "def save_comparison_matrix_to_excel(comparison_df, similarity_matrix, file_path, labels1, labels2):\n",
    "    \"\"\"Save the comparison matrix and similarity values to an Excel file.\"\"\"\n",
    "    with pd.ExcelWriter(os.path.join(output_dir, file_path)) as writer:\n",
    "        comparison_df.to_excel(writer, sheet_name='Comparison Matrix')\n",
    "\n",
    "        # Add similarity values sheet with correct dimensions\n",
    "        similarity_submatrix = similarity_matrix[:len(labels1), len(labels1):len(labels1)+len(labels2)]\n",
    "        similarity_df = pd.DataFrame(similarity_submatrix, index=labels1, columns=labels2)\n",
    "        similarity_df.to_excel(writer, sheet_name='Similarity Values')\n",
    "\n",
    "def save_embeddings_to_excel(embeddings1, embeddings2, labels1, labels2, file_path, threshold):\n",
    "    \"\"\"Save aggregated embeddings and similarity matrix to an Excel file.\"\"\"\n",
    "    \n",
    "    # Aggregate embeddings for JSON1 and JSON2 separately\n",
    "    aggregated_embeddings1 = np.array([embedding.mean(axis=0) for embedding in embeddings1])\n",
    "    aggregated_embeddings2 = np.array([embedding.mean(axis=0) for embedding in embeddings2])\n",
    "    \n",
    "    # Compute similarity matrix only for JSON1 vs JSON2\n",
    "    similarity_matrix = cosine_similarity(aggregated_embeddings1, aggregated_embeddings2)\n",
    "    \n",
    "    # Create binary match matrix\n",
    "    match_matrix = np.zeros_like(similarity_matrix)\n",
    "    \n",
    "    # Find maximum similarities above the threshold and update match matrix\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        max_sim_index = np.argmax(similarity_matrix[i])\n",
    "        if similarity_matrix[i, max_sim_index] > threshold:\n",
    "            match_matrix[i, max_sim_index] = 1\n",
    "    \n",
    "    for j in range(similarity_matrix.shape[1]):\n",
    "        max_sim_index = np.argmax(similarity_matrix[:, j])\n",
    "        if similarity_matrix[max_sim_index, j] > threshold:\n",
    "            match_matrix[max_sim_index, j] = 1\n",
    "    \n",
    "    # Create DataFrames for similarity and match matrices\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=labels1, columns=labels2)\n",
    "    match_df = pd.DataFrame(match_matrix, index=labels1, columns=labels2)\n",
    "    \n",
    "    # Save to Excel\n",
    "    with pd.ExcelWriter(os.path.join(output_dir, file_path)) as writer:\n",
    "        similarity_df.to_excel(writer, sheet_name='Similarity Matrix')\n",
    "        match_df.to_excel(writer, sheet_name='Match Matrix')\n",
    "\n",
    "    print(f\"Embeddings and similarity matrices saved to '{file_path}'.\")\n",
    "\n",
    "def save_jaccard_distances_to_excel(similarity_matrix, labels1, labels2, json_objects1, json_objects2, label_to_text1, label_to_text2, file_path, G1, G2):\n",
    "    \"\"\"\n",
    "    Save Jaccard distances derived from the similarity matrix to an Excel file with detailed calculations.\n",
    "    \n",
    "    Args:\n",
    "        similarity_matrix (np.ndarray): The cosine similarity matrix of the combined embeddings.\n",
    "        labels1 (list): The list of labels corresponding to the texts/embeddings from JSON 1.\n",
    "        labels2 (list): The list of labels corresponding to the texts/embeddings from JSON 2.\n",
    "        json_objects1 (list): JSON objects from the first dataset.\n",
    "        json_objects2 (list): JSON objects from the second dataset.\n",
    "        label_to_text1 (dict): Mapping of labels to texts for JSON 1.\n",
    "        label_to_text2 (dict): Mapping of labels to texts for JSON 2.\n",
    "        file_path (str): The file path to save the Excel file.\n",
    "        G1 (nx.Graph): The graph created from json_objects1.\n",
    "        G2 (nx.Graph): The graph created from json_objects2.\n",
    "    \"\"\"\n",
    "    num_texts1 = len(labels1)\n",
    "    num_texts2 = len(labels2)\n",
    "    jaccard_distances = []\n",
    "\n",
    "    # Create a similarity submatrix for JSON1 vs JSON2\n",
    "    similarity_submatrix = similarity_matrix[:num_texts1, :num_texts2]\n",
    "\n",
    "    # Ensure the submatrix has the correct shape\n",
    "    if similarity_submatrix.shape != (num_texts1, num_texts2):\n",
    "        raise ValueError(f\"Expected submatrix shape ({num_texts1}, {num_texts2}), but got {similarity_submatrix.shape}\")\n",
    "\n",
    "    for i in range(num_texts1):\n",
    "        for j in range(num_texts2):\n",
    "            intersection = similarity_submatrix[i, j]\n",
    "            union = 1  # In the context of cosine similarity, the union can be considered as 1.\n",
    "            jaccard_distance = 1 - intersection / union if union != 0 else 1\n",
    "\n",
    "            type1 = json_objects1[label_to_text1[labels1[i]]]['node_or_edge']\n",
    "            type2 = json_objects2[label_to_text2[labels2[j]]]['node_or_edge']\n",
    "\n",
    "            jaccard_distances.append({\n",
    "                'Text 1 Index': i,\n",
    "                'Text 2 Index': j,\n",
    "                'Text 1 Label': labels1[i],\n",
    "                'Text 2 Label': labels2[j],\n",
    "                'Type 1': type1,\n",
    "                'Type 2': type2,\n",
    "                'Intersection (Cosine Similarity)': intersection,\n",
    "                'Union': union,\n",
    "                'Jaccard Distance': jaccard_distance\n",
    "            })\n",
    "\n",
    "    jaccard_df = pd.DataFrame(jaccard_distances)\n",
    "\n",
    "    node_summary = jaccard_summary(\n",
    "        set(obj['encodeID'] for obj in json_objects1.values()),\n",
    "        set(obj['encodeID'] for obj in json_objects2.values()),\n",
    "        'Node Distance'\n",
    "    )\n",
    "\n",
    "    key_summary = jaccard_summary(\n",
    "        set(k for obj in json_objects1.values() for k in obj.keys()),\n",
    "        set(k for obj in json_objects2.values() for k in obj.keys()),\n",
    "        'Key Distance'\n",
    "    )\n",
    "\n",
    "    value_summary = jaccard_summary(\n",
    "        set(v for obj in json_objects1.values() for v in obj.values() if isinstance(v, str)),\n",
    "        set(v for obj in json_objects2.values() for v in obj.values() if isinstance(v, str)),\n",
    "        'Value Distance'\n",
    "    )\n",
    "\n",
    "    graph_summary = jaccard_summary(\n",
    "        set(G1.edges()),\n",
    "        set(G2.edges()),\n",
    "        'Graph Distance'\n",
    "    )\n",
    "\n",
    "    summary_df = pd.DataFrame([node_summary, key_summary, value_summary, graph_summary])\n",
    "\n",
    "    # Create a DataFrame for the cosine similarity matrix between JSON1 and JSON2 objects\n",
    "    cosine_df = pd.DataFrame(similarity_submatrix, index=labels1, columns=labels2)\n",
    "\n",
    "    nodes_details = []\n",
    "    edges_details = []\n",
    "\n",
    "    for i in range(num_texts1):\n",
    "        for j in range(num_texts2):\n",
    "            type1 = json_objects1[label_to_text1[labels1[i]]]['node_or_edge']\n",
    "            type2 = json_objects2[label_to_text2[labels2[j]]]['node_or_edge']\n",
    "            if type1 == 'node' and type2 == 'node':\n",
    "                nodes_details.append({\n",
    "                    'Text 1 Index': i,\n",
    "                    'Text 2 Index': j,\n",
    "                    'Text 1 Label': labels1[i],\n",
    "                    'Text 2 Label': labels2[j],\n",
    "                    'Cosine Similarity': similarity_submatrix[i, j]\n",
    "                })\n",
    "            elif type1 == 'edge' and type2 == 'edge':\n",
    "                edges_details.append({\n",
    "                    'Text 1 Index': i,\n",
    "                    'Text 2 Index': j,\n",
    "                    'Text 1 Label': labels1[i],\n",
    "                    'Text 2 Label': labels2[j],\n",
    "                    'Cosine Similarity': similarity_submatrix[i, j]\n",
    "                })\n",
    "\n",
    "    nodes_df = pd.DataFrame(nodes_details)\n",
    "    edges_df = pd.DataFrame(edges_details)\n",
    "\n",
    "    with pd.ExcelWriter(file_path) as writer:\n",
    "        cosine_df.to_excel(writer, sheet_name='Cosine Similarity')\n",
    "        nodes_df.to_excel(writer, sheet_name='Nodes Details', index=False)\n",
    "        edges_df.to_excel(writer, sheet_name='Edges Details', index=False)\n",
    "        jaccard_df.to_excel(writer, sheet_name='Jaccard Distances', index=False)\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "    print(f\"Jaccard distances saved to '{file_path}'.\")\n",
    "\n",
    "def jaccard_summary(set1, set2, label):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    jaccard_distance = 1 - intersection / union if union != 0 else 1\n",
    "    return {\n",
    "        'Metric': f'{label} (Jaccard)',\n",
    "        'Distance': jaccard_distance,\n",
    "        'Intersection': intersection,\n",
    "        'Union': union\n",
    "    }\n",
    "\n",
    "def calculate_detailed_distances(json_objects1, json_objects2, threshold=0.5):\n",
    "    def jaccard_similarity(set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return intersection / union if union != 0 else 0\n",
    "\n",
    "    def calculate_set_distance(set1, set2):\n",
    "        \"\"\"Calculate insertion, deletion, and substitution distances.\"\"\"\n",
    "        intersection = set1 & set2\n",
    "        insertion = set2 - intersection\n",
    "        deletion = set1 - intersection\n",
    "        num_substitutions = min(len(insertion), len(deletion))\n",
    "        insertion = insertion - set(list(insertion)[:num_substitutions])\n",
    "        deletion = deletion - set(list(deletion)[:num_substitutions])\n",
    "        return len(insertion), len(deletion), num_substitutions\n",
    "\n",
    "    # Nodes and Edges\n",
    "    nodes1 = {obj['encodeID'] for obj in json_objects1.values() if obj['node_or_edge'] == 'node'}\n",
    "    nodes2 = {obj['encodeID'] for obj in json_objects2.values() if obj['node_or_edge'] == 'node'}\n",
    "    edges1 = {obj['encodeID'] for obj in json_objects1.values() if obj['node_or_edge'] == 'edge'}\n",
    "    edges2 = {obj['encodeID'] for obj in json_objects2.values() if obj['node_or_edge'] == 'edge'}\n",
    "    \n",
    "    # Calculate node distances with similarity check\n",
    "    node_i, node_d, node_s = 0, 0, 0\n",
    "    matched_nodes = set()\n",
    "    \n",
    "    for n1 in nodes1:\n",
    "        obj1 = next((obj for obj in json_objects1.values() if obj['encodeID'] == n1), None)\n",
    "        for n2 in nodes2:\n",
    "            obj2 = next((obj for obj in json_objects2.values() if obj['encodeID'] == n2), None)\n",
    "            if obj1 and obj2:\n",
    "                sim = jaccard_similarity(set(obj1.items()), set(obj2.items()))\n",
    "                if sim >= threshold:\n",
    "                    matched_nodes.add((n1, n2))\n",
    "                    break\n",
    "        else:\n",
    "            node_d += 1\n",
    "    \n",
    "    for n2 in nodes2:\n",
    "        if not any(n2 == match[1] for match in matched_nodes):\n",
    "            node_i += 1\n",
    "\n",
    "    node_s = min(node_i, node_d)\n",
    "    node_i -= node_s\n",
    "    node_d -= node_s\n",
    "    node_distance = node_i + node_d + node_s\n",
    "\n",
    "    # Calculate edge distances with similarity check\n",
    "    edge_i, edge_d, edge_s = 0, 0, 0\n",
    "    matched_edges = set()\n",
    "    \n",
    "    for e1 in edges1:\n",
    "        obj1 = next((obj for obj in json_objects1.values() if obj['encodeID'] == e1), None)\n",
    "        for e2 in edges2:\n",
    "            obj2 = next((obj for obj in json_objects2.values() if obj['encodeID'] == e2), None)\n",
    "            if obj1 and obj2:\n",
    "                sim = jaccard_similarity(set(obj1.items()), set(obj2.items()))\n",
    "                if sim >= threshold:\n",
    "                    matched_edges.add((e1, e2))\n",
    "                    break\n",
    "        else:\n",
    "            edge_d += 1\n",
    "    \n",
    "    for e2 in edges2:\n",
    "        if not any(e2 == match[1] for match in matched_edges):\n",
    "            edge_i += 1\n",
    "\n",
    "    edge_s = min(edge_i, edge_d)\n",
    "    edge_i -= edge_s\n",
    "    edge_d -= edge_s\n",
    "    edge_distance = edge_i + edge_d + edge_s\n",
    "\n",
    "    if edge_distance > 0:\n",
    "        key_distance = 0\n",
    "        key_i = 0\n",
    "        key_d = 0\n",
    "        key_s = 0\n",
    "        value_distance = 0\n",
    "        value_i = 0\n",
    "        value_d = 0\n",
    "        value_s = 0\n",
    "    else:\n",
    "        # Keys and Values\n",
    "        key_i, key_d, key_s = 0, 0, 0\n",
    "        value_i, value_d, value_s = 0, 0, 0\n",
    "\n",
    "        for obj1 in json_objects1.values():\n",
    "            obj2 = next((obj for obj in json_objects2.values() if obj['encodeID'] == obj1['encodeID']), None)\n",
    "            if obj2:\n",
    "                keys1 = set(obj1.keys())\n",
    "                keys2 = set(obj2.keys())\n",
    "                ki, kd, ks = calculate_set_distance(keys1, keys2)\n",
    "                key_i += ki\n",
    "                key_d += kd\n",
    "                key_s += ks\n",
    "                \n",
    "                # Debugging\n",
    "                print(f\"Comparing objects with encodeID {obj1['encodeID']}:\")\n",
    "                print(f\"Keys1: {keys1}\")\n",
    "                print(f\"Keys2: {keys2}\")\n",
    "                print(f\"Key insertions: {ki}, deletions: {kd}, substitutions: {ks}\")\n",
    "                \n",
    "                # If there are key differences, do not check for value differences\n",
    "                if ki > 0 or kd > 0 or ks > 0:\n",
    "                    continue\n",
    "\n",
    "                for key in keys1 & keys2:\n",
    "                    if obj1[key] != obj2[key]:\n",
    "                        value_i += 1\n",
    "                        if isinstance(obj1[key], str) or isinstance(obj2[key], str):\n",
    "                            value_d += 1\n",
    "                        # Debugging\n",
    "                        print(f\"Value difference found in key '{key}': {obj1[key]} != {obj2[key]}\")\n",
    "\n",
    "        key_s = min(key_i, key_d)\n",
    "        key_i -= key_s\n",
    "        key_d -= key_s\n",
    "        key_distance = key_i + key_d + key_s\n",
    "        \n",
    "        value_s = min(value_i, value_d)\n",
    "        value_i -= value_s\n",
    "        value_d -= value_s\n",
    "        value_distance = value_i + value_d + value_s\n",
    "\n",
    "    detailed_distance = {\n",
    "        \"node_distance\": node_distance,\n",
    "        \"node_i\": node_i,\n",
    "        \"node_d\": node_d,\n",
    "        \"node_s\": node_s,\n",
    "        \"edge_distance\": edge_distance,\n",
    "        \"edge_i\": edge_i,\n",
    "        \"edge_d\": edge_d,\n",
    "        \"edge_s\": edge_s,\n",
    "        \"key_distance\": key_distance,\n",
    "        \"key_i\": key_i,\n",
    "        \"key_d\": key_d,\n",
    "        \"key_s\": key_s,\n",
    "        \"value_distance\": value_distance,\n",
    "        \"value_i\": value_i,\n",
    "        \"value_d\": value_d,\n",
    "        \"value_s\": value_s\n",
    "    }\n",
    "    \n",
    "    return [detailed_distance]\n",
    "\n",
    "\n",
    "def main():\n",
    "    global threshold\n",
    "    path1 = input(\"Enter the file path for JSON 1: \").strip('\"')\n",
    "    path2 = input(\"Enter the file path for JSON 2: \").strip('\"')\n",
    "        \n",
    "    threshold = float(input(\"Enter the similarity threshold (e.g., 0.95): \").strip())\n",
    "\n",
    "    data1 = load_json(path1)\n",
    "    if data1 is None:\n",
    "        return\n",
    "\n",
    "    data2 = load_json(path2)\n",
    "    if data2 is None:\n",
    "        return\n",
    "\n",
    "    texts1, embeddings1, tokenized_texts1, json_objects1, original_ids1, labels1 = json_to_text(data1, 'json1')\n",
    "    texts2, embeddings2, tokenized_texts2, json_objects2, original_ids2, labels2 = json_to_text(data2, 'json2')\n",
    "\n",
    "    combined_tokenized_texts = tokenized_texts1 + tokenized_texts2\n",
    "\n",
    "    save_tokenized_texts_combined(combined_tokenized_texts, 'combined')\n",
    "    save_embeddings_combined(embeddings1 + embeddings2, 'combined')\n",
    "\n",
    "    # Save embeddings and similarity for JSON1 vs JSON2\n",
    "    save_embeddings_to_excel(embeddings1, embeddings2, labels1, labels2, 'embeddings_and_similarity.xlsx', threshold)\n",
    "\n",
    "    json_objects1, json_objects2, comparison_matrix, combined_embeddings = assign_encode_ids(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, original_ids1, original_ids2, threshold)\n",
    "\n",
    "    json_objects1 = identify_node_or_edge_and_add_key(json_objects1)\n",
    "    json_objects2 = identify_node_or_edge_and_add_key(json_objects2)\n",
    "\n",
    "    # Save normalized data to JSON files\n",
    "    save_json(json_objects1, os.path.join(output_dir, 'normalized_data1.json'))\n",
    "    save_json(json_objects2, os.path.join(output_dir, 'normalized_data2.json'))\n",
    "\n",
    "    comparison_df, similarity_matrix, label_to_text1, label_to_text2 = create_comparison_matrix(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, threshold)\n",
    "    save_comparison_matrix_to_excel(comparison_df, similarity_matrix, 'comparison_matrix.xlsx', labels1, labels2)\n",
    "\n",
    "    print(\"Comparison matrix saved to 'output_files/comparison_matrix.xlsx'.\")\n",
    "\n",
    "    print(\"Embedding Details:\")\n",
    "    for i, text in enumerate(texts1 + texts2):\n",
    "        print(f\"Text {i + 1}: {text}\")\n",
    "        print(f\"Embedding: {combined_embeddings[i]}\")\n",
    "        print(f\"Dimensions: {combined_embeddings[i].shape}\")\n",
    "\n",
    "    G1 = build_graph(json_objects1, '_blue', 'blue')\n",
    "    G2 = build_graph(json_objects2, '_red', 'red')\n",
    "\n",
    "    visualize_graph(G1)\n",
    "    visualize_graph(G2)\n",
    "\n",
    "    calculate_distances(json_objects1, json_objects2)\n",
    "\n",
    "    # Correct the similarity matrix slicing here\n",
    "    sub_similarity_matrix = similarity_matrix[:len(labels1), len(labels1):len(labels1)+len(labels2)]\n",
    "    save_jaccard_distances_to_excel(sub_similarity_matrix, labels1, labels2, json_objects1, json_objects2, label_to_text1, label_to_text2, os.path.join(output_dir, 'jaccard_distances.xlsx'), G1, G2)\n",
    "\n",
    "    # Calculate detailed distances and save to JSON\n",
    "    detailed_distances = calculate_detailed_distances(json_objects1, json_objects2)\n",
    "    save_json(detailed_distances, os.path.join(output_dir, 'detailed_distances.json'))\n",
    "    print(\"Detailed distances saved to 'output_files/detailed_distances.json'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
