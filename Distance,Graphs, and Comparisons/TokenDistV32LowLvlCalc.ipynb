{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized json1 object 1: ['name', 'node', 'a', 'description', 'this', 'is', 'node', 'a', 'file', 'j', '##son', '##1']\n",
      "Embedding shape for json1 object 1: (1, 768)\n",
      "Tokenized json1 object 2: ['name', 'node', 'b', 'description', 'this', 'is', 'node', 'b', 'file', 'j', '##son', '##1']\n",
      "Embedding shape for json1 object 2: (1, 768)\n",
      "Tokenized json1 object 3: ['source', '##re', '##f', '1', 'target', '##re', '##f', '2', 'file', 'j', '##son', '##1']\n",
      "Embedding shape for json1 object 3: (1, 768)\n",
      "Tokenized json1 object 4: ['type', 'attack', '##pa', '##tter', '##n', 'name', 'http', 'flood', 'external', '##re', '##ference', '##s', 'source', '##name', 'cape', '##c', 'ur', '##l', 'https', '##cape', '##cm', '##it', '##re', '##org', '##da', '##tad', '##ef', '##ini', '##tions', '##48', '##8', '##ht', '##ml', 'external', '##id', 'cape', '##c', '##48', '##8', 'file', 'j', '##son', '##1']\n",
      "Embedding shape for json1 object 4: (1, 768)\n",
      "Tokenized json2 object 1: ['name', 'node', 'a', 'description', 'this', 'is', 'node', 'a', 'file', 'j', '##son', '##2']\n",
      "Embedding shape for json2 object 1: (1, 768)\n",
      "Tokenized json2 object 2: ['name', 'node', 'b', 'description', 'this', 'is', 'node', 'b', 'file', 'j', '##son', '##2']\n",
      "Embedding shape for json2 object 2: (1, 768)\n",
      "Tokenized json2 object 3: ['source', '##re', '##f', '1', 'target', '##re', '##f', '2', 'file', 'j', '##son', '##2']\n",
      "Embedding shape for json2 object 3: (1, 768)\n",
      "Tokenized json2 object 4: ['type', 'attack', '##pa', '##tter', '##n', 'name', '##name', '##name', 'http', 'flood', 'external', '##re', '##ference', '##s', 'source', '##name', '##so', '##ur', '##cen', '##ame', 'cape', '##c', 'ur', '##l', 'https', '##cape', '##cm', '##it', '##re', '##org', '##da', '##tad', '##ef', '##ini', '##tions', '##48', '##8', '##ht', '##ml', 'external', '##id', '53', '##56', '##56', '##64', '##34', 'file', 'j', '##son', '##2']\n",
      "Embedding shape for json2 object 4: (1, 768)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 659\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetailed distances saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_files/detailed_distances.json\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 659\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[2], line 622\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    618\u001b[0m save_embeddings_combined(embeddings1 \u001b[38;5;241m+\u001b[39m embeddings2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    620\u001b[0m json_objects1, json_objects2, comparison_matrix, combined_embeddings \u001b[38;5;241m=\u001b[39m assign_encode_ids(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, original_ids1, original_ids2, threshold)\n\u001b[1;32m--> 622\u001b[0m json_objects1 \u001b[38;5;241m=\u001b[39m identify_node_or_edge_and_add_key(json_objects1)\n\u001b[0;32m    623\u001b[0m json_objects2 \u001b[38;5;241m=\u001b[39m identify_node_or_edge_and_add_key(json_objects2)\n\u001b[0;32m    625\u001b[0m save_embeddings_to_excel(embeddings1, embeddings2, \n\u001b[0;32m    626\u001b[0m     [obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencodeID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m json_objects1\u001b[38;5;241m.\u001b[39mvalues()],\n\u001b[0;32m    627\u001b[0m     [obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencodeID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m json_objects2\u001b[38;5;241m.\u001b[39mvalues()], \n\u001b[0;32m    628\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings_and_similarity.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m, threshold)\n",
      "Cell \u001b[1;32mIn[2], line 171\u001b[0m, in \u001b[0;36midentify_node_or_edge_and_add_key\u001b[1;34m(json_objects)\u001b[0m\n\u001b[0;32m    168\u001b[0m encode_ids \u001b[38;5;241m=\u001b[39m {entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencodeID\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m json_objects\u001b[38;5;241m.\u001b[39mvalues()}\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m json_objects\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 171\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(obj\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values\u001b[38;5;241m.\u001b[39mintersection(encode_ids)) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    173\u001b[0m         obj[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_or_edge\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create a new directory to save all files\n",
    "output_dir = 'output_files'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define global threshold variable\n",
    "threshold = 0.8\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by lowercasing and removing punctuation.\"\"\"\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def get_embeddings(text):\n",
    "    \"\"\"Get BERT embeddings for the given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Last hidden state (embedding)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "def save_tokenized_texts_combined(tokenized_texts, file_label):\n",
    "    \"\"\"Save combined tokenized texts to a file.\"\"\"\n",
    "    with open(os.path.join(output_dir, f\"combined_tokenized_output_{file_label}.txt\"), \"w\") as f:\n",
    "        for tokenized_text in tokenized_texts:\n",
    "            f.write(\" \".join(tokenized_text) + \"\\n\")\n",
    "\n",
    "def save_embeddings_combined(embeddings, file_label):\n",
    "    \"\"\"Save combined embeddings as .npy file.\"\"\"\n",
    "    combined_embeddings = np.vstack(embeddings)\n",
    "    np.save(os.path.join(output_dir, f\"combined_embeddings_{file_label}.npy\"), combined_embeddings)\n",
    "\n",
    "def json_to_text(data, file_label):\n",
    "    \"\"\"Convert JSON entries to text for analysis and remove 'id' key. Add a 'file' key and determine node_or_edge.\"\"\"\n",
    "    texts = []\n",
    "    embeddings = []\n",
    "    tokenized_texts = []\n",
    "    json_objects = {}\n",
    "    original_ids = {}\n",
    "    for i, entry in enumerate(data):\n",
    "        original_id = entry.pop('id', None)  # Remove 'id' key and store it\n",
    "        if original_id:\n",
    "            original_ids[original_id] = entry  # Map original 'id' to entry\n",
    "\n",
    "        entry['file'] = file_label  # Add 'file' key\n",
    "        text = \". \".join(f\"{key}: {value}\" if not isinstance(value, list) else f\"{key}: \" + \", \".join(map(str, value)) for key, value in entry.items())\n",
    "        text = preprocess_text(text)\n",
    "        texts.append(text)\n",
    "\n",
    "        # Tokenization\n",
    "        tokenized_text = tokenizer.tokenize(text)\n",
    "        print(f\"Tokenized {file_label} object {i + 1}: {tokenized_text}\")\n",
    "        tokenized_texts.append(tokenized_text)\n",
    "\n",
    "        # Get and save embeddings\n",
    "        embedding = get_embeddings(text)\n",
    "        print(f\"Embedding shape for {file_label} object {i + 1}: {embedding.shape}\")\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "        # Determine node_or_edge\n",
    "        if 'source_ref' in entry and 'target_ref' in entry:\n",
    "            entry['node_or_edge'] = \"edge\"\n",
    "        else:\n",
    "            entry['node_or_edge'] = \"node\"\n",
    "        \n",
    "        json_objects[text] = {'entry': entry}\n",
    "    return texts, embeddings, tokenized_texts, json_objects, original_ids\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"Load JSON data from a file with detailed error handling.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.read()\n",
    "            return json.loads(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Failed to decode JSON from file - {file_path}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        print(f\"Faulty JSON content:\\n{data}\")\n",
    "        return None\n",
    "\n",
    "def assign_encode_ids(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, original_ids1, original_ids2, threshold):\n",
    "    combined_texts = texts1 + texts2\n",
    "    combined_embeddings = np.vstack(embeddings1 + embeddings2)\n",
    "    combined_json_objects = {**json_objects1, **json_objects2}\n",
    "\n",
    "    similarity_matrix = cosine_similarity(combined_embeddings)\n",
    "    encode_id = 1\n",
    "    unmatched_encode_id = 10000\n",
    "    encode_id_mapping = {}\n",
    "    assigned_encode_ids = set()\n",
    "    max_similarity = {}\n",
    "\n",
    "    num_texts = len(combined_texts)\n",
    "    comparison_matrix = np.zeros((num_texts, num_texts))\n",
    "\n",
    "    for i in range(num_texts):\n",
    "        for j in range(num_texts):\n",
    "            if i != j:\n",
    "                similarity = similarity_matrix[i, j]\n",
    "                comparison_matrix[i, j] = similarity\n",
    "                if similarity > threshold and combined_json_objects[combined_texts[i]]['entry']['file'] != combined_json_objects[combined_texts[j]]['entry']['file']:\n",
    "                    if combined_texts[i] not in assigned_encode_ids or similarity > max_similarity.get(combined_texts[i], 0):\n",
    "                        encode_id_mapping[combined_texts[i]] = encode_id\n",
    "                        encode_id_mapping[combined_texts[j]] = encode_id\n",
    "                        assigned_encode_ids.add(combined_texts[i])\n",
    "                        assigned_encode_ids.add(combined_texts[j])\n",
    "                        max_similarity[combined_texts[i]] = similarity\n",
    "                        max_similarity[combined_texts[j]] = similarity\n",
    "                        encode_id += 1\n",
    "\n",
    "    for text in combined_texts:\n",
    "        if text not in encode_id_mapping:\n",
    "            encode_id_mapping[text] = unmatched_encode_id\n",
    "            unmatched_encode_id += 1\n",
    "\n",
    "    json_objects1 = {}\n",
    "    json_objects2 = {}\n",
    "\n",
    "    for text, obj in combined_json_objects.items():\n",
    "        obj['entry']['encodeID'] = encode_id_mapping[text]\n",
    "        if obj['entry']['file'] == 'json1':\n",
    "            json_objects1[encode_id_mapping[text]] = obj['entry']\n",
    "        else:\n",
    "            json_objects2[encode_id_mapping[text]] = obj['entry']\n",
    "\n",
    "    # Create a dictionary to map original IDs to encodeIDs\n",
    "    id_to_encodeID = {original_id: obj['encodeID'] for original_id, obj in original_ids1.items()}\n",
    "    id_to_encodeID.update({original_id: obj['encodeID'] for original_id, obj in original_ids2.items()})\n",
    "\n",
    "    # Update source_ref and target_ref in relationship objects\n",
    "    for obj in json_objects1.values():\n",
    "        if obj['node_or_edge'] == 'edge':\n",
    "            obj['source_ref'] = id_to_encodeID.get(obj['source_ref'], obj['source_ref'])\n",
    "            obj['target_ref'] = id_to_encodeID.get(obj['target_ref'], obj['target_ref'])\n",
    "\n",
    "    for obj in json_objects2.values():\n",
    "        if obj['node_or_edge'] == 'edge':\n",
    "            obj['source_ref'] = id_to_encodeID.get(obj['source_ref'], obj['source_ref'])\n",
    "            obj['target_ref'] = id_to_encodeID.get(obj['target_ref'], obj['target_ref'])\n",
    "\n",
    "    return json_objects1, json_objects2, comparison_matrix, combined_embeddings\n",
    "\n",
    "def identify_node_or_edge_and_add_key(json_objects):\n",
    "    \"\"\"Identify if an object is a node or an edge based on the presence of two values matching encodeIDs of other objects,\n",
    "    and add a new key-value pair indicating its type.\"\"\"\n",
    "    \n",
    "    encode_ids = {entry['encodeID'] for entry in json_objects.values()}\n",
    "    \n",
    "    for obj in json_objects.values():\n",
    "        values = set(obj.values())\n",
    "        if len(values.intersection(encode_ids)) >= 2:\n",
    "            obj['node_or_edge'] = \"edge\"\n",
    "        else:\n",
    "            obj['node_or_edge'] = \"node\"\n",
    "    \n",
    "    return json_objects\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    \"\"\"Save JSON data to a file.\"\"\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def build_graph(json_objects, suffix, color):\n",
    "    \"\"\"Build a graph based on JSON objects.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    valid_nodes = set()\n",
    "\n",
    "    # Add nodes, ensuring unique IDs\n",
    "    for key, obj in json_objects.items():\n",
    "        if obj['node_or_edge'] != 'edge':  # Only add entities as nodes\n",
    "            if obj.get('name') != 'Unknown Name':  # Avoid adding unknown name nodes\n",
    "                unique_id = str(obj.get('encodeID')) + suffix\n",
    "                node_name = f\"{obj.get('encodeID')} ({obj['file']})\"\n",
    "                G.add_node(unique_id, description=obj.get('description', 'No description provided'))\n",
    "                valid_nodes.add(unique_id)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Add edges, using unique IDs\n",
    "    unique_edges = set()\n",
    "    for obj in json_objects.values():\n",
    "        if obj['node_or_edge'] == 'edge':\n",
    "            source = str(obj.get('source_ref')) + suffix\n",
    "            target = str(obj.get('target_ref')) + suffix\n",
    "            edge = (source, target)\n",
    "            if source in valid_nodes and target in valid_nodes and source != target:  # Ensure both nodes are valid and not the same\n",
    "                G.add_edge(source, target)\n",
    "                unique_edges.add(edge)\n",
    "    return G\n",
    "\n",
    "\n",
    "def visualize_graph(G):\n",
    "    \"\"\"Visualize a networkx graph with enhanced label formatting for readability.\"\"\"\n",
    "    num_nodes = len(G.nodes)\n",
    "    figsize = get_optimal_figsize(num_nodes)\n",
    "    k = get_optimal_k(num_nodes)\n",
    "\n",
    "    plt.figure(figsize=figsize)  # Dynamically set figure size\n",
    "    pos = nx.spring_layout(G, k=k)  # Dynamically set layout parameter\n",
    "\n",
    "    # Draw nodes with their corresponding colors\n",
    "    node_colors = [data.get('color', 'grey') for node, data in G.nodes(data=True)]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=300)\n",
    "\n",
    "    # Draw edges with specified styles and colors\n",
    "    for (u, v, data) in G.edges(data=True):\n",
    "        edge_style = data.get('style', 'dotted')\n",
    "        edge_color = data.get('color', 'black')\n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos, edgelist=[(u, v)],\n",
    "            style=edge_style,\n",
    "            edge_color=edge_color,\n",
    "            width=2\n",
    "        )\n",
    "\n",
    "    # Draw edge labels\n",
    "    edge_labels = {(u, v): data.get('label', 'similar-to') for u, v, data in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "    # Draw labels for nodes\n",
    "    labels = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        label = f\"{data.get('label', 'Unknown')}\\n({data.get('type', 'Unknown')})\"\n",
    "        description = data.get('description', 'No description provided')\n",
    "        if description:\n",
    "            doc = nlp(description)\n",
    "            verbs = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "            label += f\"\\n{' '.join(verbs[:2])}\"\n",
    "        labels[node] = label\n",
    "\n",
    "    for node, label in labels.items():\n",
    "        x, y = pos[node]\n",
    "        plt.text(x, y, label, fontsize=9, ha='center', va='center',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='white', edgecolor='gray', alpha=0.6))\n",
    "\n",
    "    plt.title('Graph Visualization')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_optimal_figsize(num_nodes):\n",
    "    \"\"\"Determine an optimal figure size based on the number of nodes.\"\"\"\n",
    "    base_size = 10  # Base size for the figure\n",
    "    scale_factor = 0.5  # Scale factor to adjust size\n",
    "    return (base_size + num_nodes * scale_factor, base_size + num_nodes * scale_factor)\n",
    "\n",
    "def get_optimal_k(num_nodes):\n",
    "    \"\"\"Determine an optimal k value for spring_layout based on the number of nodes.\"\"\"\n",
    "    base_k = 0.5  # Base k value\n",
    "    scale_factor = 0.1  # Scale factor to adjust k\n",
    "    return base_k + num_nodes * scale_factor\n",
    "\n",
    "# Define node substitution cost function\n",
    "def node_subst_cost(n1, n2):\n",
    "    \"\"\"Define cost for node substitution.\"\"\"\n",
    "    return 0 if n1 == n2 else 1\n",
    "\n",
    "# Define edge substitution cost function\n",
    "def edge_subst_cost(e1, e2):\n",
    "    \"\"\"Define cost for edge substitution.\"\"\"\n",
    "    return 0 if e1 == e2 else 1\n",
    "\n",
    "def calculate_distances(json_objects1, json_objects2):\n",
    "    \"\"\"Calculate node distance, key distance, value distance, and graph distance.\"\"\"\n",
    "    def jaccard_distance(set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return 1 - intersection / union if union != 0 else 1\n",
    "\n",
    "    def set_distance(set1, set2):\n",
    "        return abs(len(set1) - len(set2))\n",
    "\n",
    "    # Node distance\n",
    "    nodes1 = set(json_objects1.keys())\n",
    "    nodes2 = set(json_objects2.keys())\n",
    "    node_distance = set_distance(nodes1, nodes2)\n",
    "    node_jaccard = jaccard_distance(nodes1, nodes2)\n",
    "\n",
    "    # Key distance\n",
    "    keys1 = set(k for obj in json_objects1.values() for k in obj.keys())\n",
    "    keys2 = set(k for obj in json_objects2.values() for k in obj.keys())\n",
    "    key_distance = set_distance(keys1, keys2)\n",
    "    key_jaccard = jaccard_distance(keys1, keys2)\n",
    "\n",
    "    # Value distance\n",
    "    values1 = set(v for obj in json_objects1.values() for v in obj.values() if isinstance(v, str))\n",
    "    values2 = set(v for obj in json_objects2.values() for v in obj.values() if isinstance(v, str))\n",
    "    value_distance = set_distance(values1, values2)\n",
    "    value_jaccard = jaccard_distance(values1, values2)\n",
    "\n",
    "    # Graph distance (using node and edge comparison)\n",
    "    G1 = build_graph(json_objects1, '_blue', 'blue')\n",
    "    G2 = build_graph(json_objects2, '_red', 'red')\n",
    "    graph_distance = nx.graph_edit_distance(G1, G2, node_subst_cost=node_subst_cost, edge_subst_cost=edge_subst_cost)\n",
    "\n",
    "    print(f\"Node Distance (Simple): {node_distance}\")\n",
    "    print(f\"Node Distance (Jaccard): {node_jaccard:.2f}\")\n",
    "    print(f\"Key Distance (Simple): {key_distance}\")\n",
    "    print(f\"Key Distance (Jaccard): {key_jaccard:.2f}\")\n",
    "    print(f\"Value Distance (Simple): {value_distance}\")\n",
    "    print(f\"Value Distance (Jaccard): {value_jaccard:.2f}\")\n",
    "    print(f\"Graph Distance: {graph_distance:.2f}\")\n",
    "\n",
    "def create_comparison_matrix(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, threshold):\n",
    "    combined_embeddings = np.vstack(embeddings1 + embeddings2)\n",
    "    similarity_matrix = cosine_similarity(combined_embeddings)\n",
    "\n",
    "    num_texts1 = len(texts1)\n",
    "    num_texts2 = len(texts2)\n",
    "    comparison_data = []\n",
    "\n",
    "    for i in range(num_texts1):\n",
    "        row = []\n",
    "        for j in range(num_texts2):\n",
    "            similarity = similarity_matrix[i, num_texts1 + j]\n",
    "            match = 1 if similarity > threshold else 0\n",
    "            row.append(match)\n",
    "        comparison_data.append(row)\n",
    "\n",
    "    labels1 = [obj['encodeID'] for obj in json_objects1.values()]\n",
    "    labels2 = [obj['encodeID'] for obj in json_objects2.values()]\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data, index=labels1, columns=labels2)\n",
    "\n",
    "    return comparison_df, similarity_matrix\n",
    "\n",
    "\n",
    "def save_comparison_matrix_to_excel(comparison_df, similarity_matrix, file_path, labels1, labels2):\n",
    "    \"\"\"Save the comparison matrix and similarity values to an Excel file.\"\"\"\n",
    "    with pd.ExcelWriter(os.path.join(output_dir, file_path)) as writer:\n",
    "        comparison_df.to_excel(writer, sheet_name='Comparison Matrix')\n",
    "\n",
    "        # Add similarity values sheet with correct dimensions\n",
    "        similarity_submatrix = similarity_matrix[:len(labels1), len(labels1):len(labels1)+len(labels2)]\n",
    "        similarity_df = pd.DataFrame(similarity_submatrix, index=labels1, columns=labels2)\n",
    "        similarity_df.to_excel(writer, sheet_name='Similarity Values')\n",
    "\n",
    "def save_embeddings_to_excel(embeddings1, embeddings2, labels1, labels2, file_path, threshold):\n",
    "    \"\"\"Save aggregated embeddings and similarity matrix to an Excel file.\"\"\"\n",
    "    \n",
    "    # Aggregate embeddings for JSON1 and JSON2 separately\n",
    "    aggregated_embeddings1 = np.array([embedding.mean(axis=0) for embedding in embeddings1])\n",
    "    aggregated_embeddings2 = np.array([embedding.mean(axis=0) for embedding in embeddings2])\n",
    "    \n",
    "    # Compute similarity matrix only for JSON1 vs JSON2\n",
    "    similarity_matrix = cosine_similarity(aggregated_embeddings1, aggregated_embeddings2)\n",
    "    \n",
    "    # Create binary match matrix\n",
    "    match_matrix = np.zeros_like(similarity_matrix)\n",
    "    \n",
    "    # Find maximum similarities above the threshold and update match matrix\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        max_sim_index = np.argmax(similarity_matrix[i])\n",
    "        if similarity_matrix[i, max_sim_index] > threshold:\n",
    "            match_matrix[i, max_sim_index] = 1\n",
    "    \n",
    "    for j in range(similarity_matrix.shape[1]):\n",
    "        max_sim_index = np.argmax(similarity_matrix[:, j])\n",
    "        if similarity_matrix[max_sim_index, j] > threshold:\n",
    "            match_matrix[max_sim_index, j] = 1\n",
    "    \n",
    "    # Create DataFrames for similarity and match matrices\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=labels1, columns=labels2)\n",
    "    match_df = pd.DataFrame(match_matrix, index=labels1, columns=labels2)\n",
    "    \n",
    "    # Save to Excel\n",
    "    with pd.ExcelWriter(os.path.join(output_dir, file_path)) as writer:\n",
    "        similarity_df.to_excel(writer, sheet_name='Similarity Matrix')\n",
    "        match_df.to_excel(writer, sheet_name='Match Matrix')\n",
    "\n",
    "    print(f\"Embeddings and similarity matrices saved to '{file_path}'.\")\n",
    "\n",
    "def save_jaccard_distances_to_excel(similarity_matrix, labels1, labels2, json_objects1, json_objects2, file_path, G1, G2):\n",
    "    num_texts1 = len(labels1)\n",
    "    num_texts2 = len(labels2)\n",
    "    jaccard_distances = []\n",
    "\n",
    "    similarity_submatrix = similarity_matrix[:num_texts1, :num_texts2]\n",
    "\n",
    "    if similarity_submatrix.shape != (num_texts1, num_texts2):\n",
    "        raise ValueError(f\"Expected submatrix shape ({num_texts1}, {num_texts2}), but got {similarity_submatrix.shape}\")\n",
    "\n",
    "    for i in range(num_texts1):\n",
    "        for j in range(num_texts2):\n",
    "            intersection = similarity_submatrix[i, j]\n",
    "            union = 1\n",
    "            jaccard_distance = 1 - intersection / union if union != 0 else 1\n",
    "\n",
    "            # Ensure encodeID exists before accessing node_or_edge\n",
    "            try:\n",
    "                type1 = json_objects1[labels1[i]]['node_or_edge']\n",
    "                type2 = json_objects2[labels2[j]]['node_or_edge']\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError: {e} for labels1[{i}]: {labels1[i]} and labels2[{j}]: {labels2[j]}\")\n",
    "                continue\n",
    "\n",
    "            jaccard_distances.append({\n",
    "                'Text 1 Index': i,\n",
    "                'Text 2 Index': j,\n",
    "                'Text 1 Label': labels1[i],\n",
    "                'Text 2 Label': labels2[j],\n",
    "                'Type 1': type1,\n",
    "                'Type 2': type2,\n",
    "                'Intersection (Cosine Similarity)': intersection,\n",
    "                'Union': union,\n",
    "                'Jaccard Distance': jaccard_distance\n",
    "            })\n",
    "\n",
    "    jaccard_df = pd.DataFrame(jaccard_distances)\n",
    "\n",
    "    node_summary = jaccard_summary(\n",
    "        set(obj['encodeID'] for obj in json_objects1.values()),\n",
    "        set(obj['encodeID'] for obj in json_objects2.values()),\n",
    "        'Node Distance'\n",
    "    )\n",
    "\n",
    "    key_summary = jaccard_summary(\n",
    "        set(k for obj in json_objects1.values() for k in obj.keys()),\n",
    "        set(k for obj in json_objects2.values() for k in obj.keys()),\n",
    "        'Key Distance'\n",
    "    )\n",
    "\n",
    "    value_summary = jaccard_summary(\n",
    "        set(v for obj in json_objects1.values() for v in obj.values() if isinstance(v, str)),\n",
    "        set(v for obj in json_objects2.values() for v in obj.values() if isinstance(v, str)),\n",
    "        'Value Distance'\n",
    "    )\n",
    "\n",
    "    graph_summary = jaccard_summary(\n",
    "        set(G1.edges()),\n",
    "        set(G2.edges()),\n",
    "        'Graph Distance'\n",
    "    )\n",
    "\n",
    "    summary_df = pd.DataFrame([node_summary, key_summary, value_summary, graph_summary])\n",
    "\n",
    "    cosine_df = pd.DataFrame(similarity_submatrix, index=labels1, columns=labels2)\n",
    "\n",
    "    nodes_details = []\n",
    "    edges_details = []\n",
    "\n",
    "    for i in range(num_texts1):\n",
    "        for j in range(num_texts2):\n",
    "            type1 = json_objects1[labels1[i]]['node_or_edge']\n",
    "            type2 = json_objects2[labels2[j]]['node_or_edge']\n",
    "            if type1 == 'node' and type2 == 'node':\n",
    "                nodes_details.append({\n",
    "                    'Text 1 Index': i,\n",
    "                    'Text 2 Index': j,\n",
    "                    'Text 1 Label': labels1[i],\n",
    "                    'Text 2 Label': labels2[j],\n",
    "                    'Cosine Similarity': similarity_submatrix[i, j]\n",
    "                })\n",
    "            elif type1 == 'edge' and type2 == 'edge':\n",
    "                edges_details.append({\n",
    "                    'Text 1 Index': i,\n",
    "                    'Text 2 Index': j,\n",
    "                    'Text 1 Label': labels1[i],\n",
    "                    'Text 2 Label': labels2[j],\n",
    "                    'Cosine Similarity': similarity_submatrix[i, j]\n",
    "                })\n",
    "\n",
    "    nodes_df = pd.DataFrame(nodes_details)\n",
    "    edges_df = pd.DataFrame(edges_details)\n",
    "\n",
    "    with pd.ExcelWriter(file_path) as writer:\n",
    "        cosine_df.to_excel(writer, sheet_name='Cosine Similarity')\n",
    "        nodes_df.to_excel(writer, sheet_name='Nodes Details', index=False)\n",
    "        edges_df.to_excel(writer, sheet_name='Edges Details', index=False)\n",
    "        jaccard_df.to_excel(writer, sheet_name='Jaccard Distances', index=False)\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "    print(f\"Jaccard distances saved to '{file_path}'.\")\n",
    "\n",
    "def jaccard_summary(set1, set2, label):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    jaccard_distance = 1 - intersection / union if union != 0 else 1\n",
    "    return {\n",
    "        'Metric': f'{label} (Jaccard)',\n",
    "        'Distance': jaccard_distance,\n",
    "        'Intersection': intersection,\n",
    "        'Union': union\n",
    "    }\n",
    "\n",
    "def calculate_detailed_distances(json_objects1, json_objects2, threshold):\n",
    "    node_i, node_d, node_s = 0, 0, 0\n",
    "    edge_i, edge_d, edge_s = 0, 0, 0\n",
    "    key_i, key_d, key_s = 0, 0, 0\n",
    "    value_i, value_d, value_s = 0, 0, 0\n",
    "\n",
    "    # Identify nodes and edges\n",
    "    nodes1 = {k: v for k, v in json_objects1.items() if v['node_or_edge'] == 'node'}\n",
    "    nodes2 = {k: v for k, v in json_objects2.items() if v['node_or_edge'] == 'node'}\n",
    "    edges1 = {k: v for k, v in json_objects1.items() if v['node_or_edge'] == 'edge'}\n",
    "    edges2 = {k: v for k, v in json_objects2.items() if v['node_or_edge'] == 'edge'}\n",
    "\n",
    "    # Debug: Print the nodes and edges\n",
    "    print(\"Nodes in JSON1:\", list(nodes1.keys()))\n",
    "    print(\"Nodes in JSON2:\", list(nodes2.keys()))\n",
    "    print(\"Edges in JSON1:\", list(edges1.keys()))\n",
    "    print(\"Edges in JSON2:\", list(edges2.keys()))\n",
    "\n",
    "    # Compare nodes\n",
    "    nodes_only_in_1 = set(nodes1.keys()) - set(nodes2.keys())\n",
    "    nodes_only_in_2 = set(nodes2.keys()) - set(nodes1.keys())\n",
    "    node_s = min(len(nodes_only_in_1), len(nodes_only_in_2))\n",
    "    node_d = len(nodes_only_in_1) - node_s\n",
    "    node_i = len(nodes_only_in_2) - node_s\n",
    "\n",
    "    print(f\"Node substitutions: {node_s}\")\n",
    "    print(f\"Node deletions: {node_d}\")\n",
    "    print(f\"Node insertions: {node_i}\")\n",
    "\n",
    "    # Compare edges based on their content, not just IDs\n",
    "    edges1_set = {(e['source_ref'], e['target_ref']) for e in edges1.values()}\n",
    "    edges2_set = {(e['source_ref'], e['target_ref']) for e in edges2.values()}\n",
    "    \n",
    "    edges_only_in_1 = edges1_set - edges2_set\n",
    "    edges_only_in_2 = edges2_set - edges1_set\n",
    "    \n",
    "    edge_d = len(edges_only_in_1)\n",
    "    edge_i = len(edges_only_in_2)\n",
    "    edge_s = 0  # There are no substitutions for edges, only insertions and deletions\n",
    "\n",
    "    print(f\"Edge substitutions: {edge_s}\")\n",
    "    print(f\"Edge deletions: {edge_d}\")\n",
    "    print(f\"Edge insertions: {edge_i}\")\n",
    "\n",
    "    # Compare keys and values for nodes present in both JSONs\n",
    "    for node_id in set(nodes1.keys()) & set(nodes2.keys()):\n",
    "        obj1, obj2 = nodes1[node_id], nodes2[node_id]\n",
    "        keys1, keys2 = set(obj1.keys()) - {'file'}, set(obj2.keys()) - {'file'}\n",
    "        \n",
    "        keys_only_in_1 = keys1 - keys2\n",
    "        keys_only_in_2 = keys2 - keys1\n",
    "        key_s += min(len(keys_only_in_1), len(keys_only_in_2))\n",
    "        key_d += len(keys_only_in_1) - key_s\n",
    "        key_i += len(keys_only_in_2) - key_s\n",
    "\n",
    "        for key in keys1 & keys2:\n",
    "            if key != 'file' and obj1[key] != obj2[key]:\n",
    "                value_s += 1\n",
    "\n",
    "    print(f\"Key substitutions: {key_s}\")\n",
    "    print(f\"Key deletions: {key_d}\")\n",
    "    print(f\"Key insertions: {key_i}\")\n",
    "    print(f\"Value substitutions: {value_s}\")\n",
    "\n",
    "    # Calculate distances\n",
    "    node_distance = node_i + node_d + node_s\n",
    "    edge_distance = edge_i + edge_d + edge_s\n",
    "    key_distance = key_i + key_d + key_s\n",
    "    value_distance = value_s  # value_i and value_d are not used in this logic\n",
    "\n",
    "    detailed_distance = [{\n",
    "        \"node_distance\": node_distance,\n",
    "        \"node_i\": node_i,\n",
    "        \"node_d\": node_d,\n",
    "        \"node_s\": node_s,\n",
    "        \"edge_distance\": edge_distance,\n",
    "        \"edge_i\": edge_i,\n",
    "        \"edge_d\": edge_d,\n",
    "        \"edge_s\": edge_s,\n",
    "        \"key_distance\": key_distance,\n",
    "        \"key_i\": key_i,\n",
    "        \"key_d\": key_d,\n",
    "        \"key_s\": key_s,\n",
    "        \"value_distance\": value_distance,\n",
    "        \"value_i\": 0,\n",
    "        \"value_d\": 0,\n",
    "        \"value_s\": value_s\n",
    "    }]\n",
    "\n",
    "    return detailed_distance\n",
    "\n",
    "def main():\n",
    "    global threshold\n",
    "    path1 = input(\"Enter the file path for JSON 1: \").strip('\"')\n",
    "    path2 = input(\"Enter the file path for JSON 2: \").strip('\"')\n",
    "        \n",
    "    threshold = float(input(\"Enter the similarity threshold (e.g., 0.95): \").strip())\n",
    "\n",
    "    data1 = load_json(path1)\n",
    "    if data1 is None:\n",
    "        return\n",
    "\n",
    "    data2 = load_json(path2)\n",
    "    if data2 is None:\n",
    "        return\n",
    "\n",
    "    texts1, embeddings1, tokenized_texts1, json_objects1, original_ids1 = json_to_text(data1, 'json1')\n",
    "    texts2, embeddings2, tokenized_texts2, json_objects2, original_ids2 = json_to_text(data2, 'json2')\n",
    "\n",
    "    combined_tokenized_texts = tokenized_texts1 + tokenized_texts2\n",
    "\n",
    "    save_tokenized_texts_combined(combined_tokenized_texts, 'combined')\n",
    "    save_embeddings_combined(embeddings1 + embeddings2, 'combined')\n",
    "\n",
    "    json_objects1, json_objects2, comparison_matrix, combined_embeddings = assign_encode_ids(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, original_ids1, original_ids2, threshold)\n",
    "\n",
    "    json_objects1 = identify_node_or_edge_and_add_key(json_objects1)\n",
    "    json_objects2 = identify_node_or_edge_and_add_key(json_objects2)\n",
    "\n",
    "    save_embeddings_to_excel(embeddings1, embeddings2, \n",
    "        [obj['encodeID'] for obj in json_objects1.values()],\n",
    "        [obj['encodeID'] for obj in json_objects2.values()], \n",
    "        'embeddings_and_similarity.xlsx', threshold)\n",
    "\n",
    "    save_json(json_objects1, os.path.join(output_dir, 'normalized_data1.json'))\n",
    "    save_json(json_objects2, os.path.join(output_dir, 'normalized_data2.json'))\n",
    "\n",
    "    comparison_df, similarity_matrix = create_comparison_matrix(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, threshold)\n",
    "    save_comparison_matrix_to_excel(comparison_df, similarity_matrix, 'comparison_matrix.xlsx', \n",
    "        [obj['encodeID'] for obj in json_objects1.values()], \n",
    "        [obj['encodeID'] for obj in json_objects2.values()])\n",
    "\n",
    "    print(\"Comparison matrix saved to 'output_files/comparison_matrix.xlsx'.\")\n",
    "\n",
    "    G1 = build_graph(json_objects1, '_blue', 'blue')\n",
    "    G2 = build_graph(json_objects2, '_red', 'red')\n",
    "\n",
    "    visualize_graph(G1)\n",
    "    visualize_graph(G2)\n",
    "\n",
    "    calculate_distances(json_objects1, json_objects2)\n",
    "\n",
    "    sub_similarity_matrix = similarity_matrix[:len(json_objects1), len(json_objects1):len(json_objects1)+len(json_objects2)]\n",
    "    save_jaccard_distances_to_excel(sub_similarity_matrix, \n",
    "        [obj['encodeID'] for obj in json_objects1.values()], \n",
    "        [obj['encodeID'] for obj in json_objects2.values()], \n",
    "        json_objects1, json_objects2, os.path.join(output_dir, 'jaccard_distances.xlsx'), G1, G2)\n",
    "\n",
    "    detailed_distances = calculate_detailed_distances(json_objects1, json_objects2, threshold)\n",
    "    save_json(detailed_distances, os.path.join(output_dir, 'detailed_distances.json'))\n",
    "    print(\"Detailed distances saved to 'output_files/detailed_distances.json'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
