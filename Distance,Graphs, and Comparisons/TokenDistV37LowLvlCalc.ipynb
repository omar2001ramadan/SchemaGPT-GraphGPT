{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing object 1 for json1:\n",
      "  Encode ID: 1\n",
      "  Type: campaign\n",
      "  Name: APT BPP\n",
      "  Node or Edge: node\n",
      "  File: json1\n",
      "Processing object 2 for json1:\n",
      "  Encode ID: 2\n",
      "  Type: intrusion-set\n",
      "  Name: APT BPP\n",
      "  Node or Edge: node\n",
      "  File: json1\n",
      "Processing object 3 for json1:\n",
      "  Encode ID: 3\n",
      "  Type: identity\n",
      "  Name: Franistan\n",
      "  Node or Edge: node\n",
      "  File: json1\n",
      "Processing object 4 for json1:\n",
      "  Encode ID: 4\n",
      "  Type: identity\n",
      "  Name: Branistan People’s Party\n",
      "  Node or Edge: node\n",
      "  File: json1\n",
      "Processing object 5 for json1:\n",
      "  Encode ID: 5\n",
      "  Type: identity\n",
      "  Name: Branistan\n",
      "  Node or Edge: node\n",
      "  File: json1\n",
      "Processing object 6 for json1:\n",
      "  Encode ID: 6\n",
      "  Type: attack-pattern\n",
      "  Name: False Information Insertion\n",
      "  Node or Edge: node\n",
      "  File: json1\n",
      "Processing object 7 for json1:\n",
      "  Encode ID: 7\n",
      "  Type: attack-pattern\n",
      "  Name: DDoS Campaign\n",
      "  Node or Edge: node\n",
      "  File: json1\n",
      "Processing object 8 for json1:\n",
      "  Encode ID: 8\n",
      "  Type: malware\n",
      "  Name: DDoS Malware\n",
      "  Node or Edge: node\n",
      "  File: json1\n",
      "Processing object 9 for json1:\n",
      "  Encode ID: 9\n",
      "  Type: indicator\n",
      "  Name: www.bpp.bn\n",
      "  Node or Edge: node\n",
      "  File: json1\n",
      "Processing object 10 for json1:\n",
      "  Encode ID: 10\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json1\n",
      "Processing object 11 for json1:\n",
      "  Encode ID: 11\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json1\n",
      "Processing object 12 for json1:\n",
      "  Encode ID: 12\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json1\n",
      "Processing object 13 for json1:\n",
      "  Encode ID: 13\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json1\n",
      "Processing object 14 for json1:\n",
      "  Encode ID: 14\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json1\n",
      "Processing object 15 for json1:\n",
      "  Encode ID: 15\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json1\n",
      "Number of objects in json1: 15\n",
      "Number of embeddings in json1: 15\n",
      "Object order in json1: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "Processing object 1 for json2:\n",
      "  Encode ID: 1\n",
      "  Type: threat-actor\n",
      "  Name: Fake BPP (Branistan Peoples Party)\n",
      "  Node or Edge: node\n",
      "  File: json2\n",
      "Processing object 2 for json2:\n",
      "  Encode ID: 2\n",
      "  Type: identity\n",
      "  Name: Franistan Intelligence\n",
      "  Node or Edge: node\n",
      "  File: json2\n",
      "Processing object 3 for json2:\n",
      "  Encode ID: 3\n",
      "  Type: identity\n",
      "  Name: Branistan Peoples Party\n",
      "  Node or Edge: node\n",
      "  File: json2\n",
      "Processing object 4 for json2:\n",
      "  Encode ID: 4\n",
      "  Type: attack-pattern\n",
      "  Name: Content Spoofing\n",
      "  Node or Edge: node\n",
      "  File: json2\n",
      "Processing object 5 for json2:\n",
      "  Encode ID: 5\n",
      "  Type: attack-pattern\n",
      "  Name: HTTP Flood\n",
      "  Node or Edge: node\n",
      "  File: json2\n",
      "Processing object 6 for json2:\n",
      "  Encode ID: 6\n",
      "  Type: campaign\n",
      "  Name: Operation Bran Flakes\n",
      "  Node or Edge: node\n",
      "  File: json2\n",
      "Processing object 7 for json2:\n",
      "  Encode ID: 7\n",
      "  Type: campaign\n",
      "  Name: Operation Raisin Bran\n",
      "  Node or Edge: node\n",
      "  File: json2\n",
      "Processing object 8 for json2:\n",
      "  Encode ID: 8\n",
      "  Type: intrusion-set\n",
      "  Name: APT BPP\n",
      "  Node or Edge: node\n",
      "  File: json2\n",
      "Processing object 9 for json2:\n",
      "  Encode ID: 9\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 10 for json2:\n",
      "  Encode ID: 10\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 11 for json2:\n",
      "  Encode ID: 11\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 12 for json2:\n",
      "  Encode ID: 12\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 13 for json2:\n",
      "  Encode ID: 13\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 14 for json2:\n",
      "  Encode ID: 14\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 15 for json2:\n",
      "  Encode ID: 15\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 16 for json2:\n",
      "  Encode ID: 16\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 17 for json2:\n",
      "  Encode ID: 17\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 18 for json2:\n",
      "  Encode ID: 18\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 19 for json2:\n",
      "  Encode ID: 19\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 20 for json2:\n",
      "  Encode ID: 20\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 21 for json2:\n",
      "  Encode ID: 21\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 22 for json2:\n",
      "  Encode ID: 22\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 23 for json2:\n",
      "  Encode ID: 23\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 24 for json2:\n",
      "  Encode ID: 24\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 25 for json2:\n",
      "  Encode ID: 25\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Processing object 26 for json2:\n",
      "  Encode ID: 26\n",
      "  Type: relationship\n",
      "  Name: No name\n",
      "  Node or Edge: edge\n",
      "  File: json2\n",
      "Number of objects in json2: 26\n",
      "Number of embeddings in json2: 26\n",
      "Object order in json2: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "Number of objects before assign_encode_ids, json_objects1: 15, json_objects2: 26\n",
      "Contents of json_objects1:\n",
      "  1: campaign - APT BPP - json1\n",
      "  2: intrusion-set - APT BPP - json1\n",
      "  3: identity - Franistan - json1\n",
      "  4: identity - Branistan People’s Party - json1\n",
      "  5: identity - Branistan - json1\n",
      "  6: attack-pattern - False Information Insertion - json1\n",
      "  7: attack-pattern - DDoS Campaign - json1\n",
      "  8: malware - DDoS Malware - json1\n",
      "  9: indicator - www.bpp.bn - json1\n",
      "  10: relationship - No name - json1\n",
      "  11: relationship - No name - json1\n",
      "  12: relationship - No name - json1\n",
      "  13: relationship - No name - json1\n",
      "  14: relationship - No name - json1\n",
      "  15: relationship - No name - json1\n",
      "Contents of json_objects2:\n",
      "  1: threat-actor - Fake BPP (Branistan Peoples Party) - json2\n",
      "  2: identity - Franistan Intelligence - json2\n",
      "  3: identity - Branistan Peoples Party - json2\n",
      "  4: attack-pattern - Content Spoofing - json2\n",
      "  5: attack-pattern - HTTP Flood - json2\n",
      "  6: campaign - Operation Bran Flakes - json2\n",
      "  7: campaign - Operation Raisin Bran - json2\n",
      "  8: intrusion-set - APT BPP - json2\n",
      "  9: relationship - No name - json2\n",
      "  10: relationship - No name - json2\n",
      "  11: relationship - No name - json2\n",
      "  12: relationship - No name - json2\n",
      "  13: relationship - No name - json2\n",
      "  14: relationship - No name - json2\n",
      "  15: relationship - No name - json2\n",
      "  16: relationship - No name - json2\n",
      "  17: relationship - No name - json2\n",
      "  18: relationship - No name - json2\n",
      "  19: relationship - No name - json2\n",
      "  20: relationship - No name - json2\n",
      "  21: relationship - No name - json2\n",
      "  22: relationship - No name - json2\n",
      "  23: relationship - No name - json2\n",
      "  24: relationship - No name - json2\n",
      "  25: relationship - No name - json2\n",
      "  26: relationship - No name - json2\n",
      "File attributes before processing:\n",
      "  json1_1: json1\n",
      "  json1_2: json1\n",
      "  json1_3: json1\n",
      "  json1_4: json1\n",
      "  json1_5: json1\n",
      "  json1_6: json1\n",
      "  json1_7: json1\n",
      "  json1_8: json1\n",
      "  json1_9: json1\n",
      "  json1_10: json1\n",
      "  json1_11: json1\n",
      "  json1_12: json1\n",
      "  json1_13: json1\n",
      "  json1_14: json1\n",
      "  json1_15: json1\n",
      "  json2_1: json2\n",
      "  json2_2: json2\n",
      "  json2_3: json2\n",
      "  json2_4: json2\n",
      "  json2_5: json2\n",
      "  json2_6: json2\n",
      "  json2_7: json2\n",
      "  json2_8: json2\n",
      "  json2_9: json2\n",
      "  json2_10: json2\n",
      "  json2_11: json2\n",
      "  json2_12: json2\n",
      "  json2_13: json2\n",
      "  json2_14: json2\n",
      "  json2_15: json2\n",
      "  json2_16: json2\n",
      "  json2_17: json2\n",
      "  json2_18: json2\n",
      "  json2_19: json2\n",
      "  json2_20: json2\n",
      "  json2_21: json2\n",
      "  json2_22: json2\n",
      "  json2_23: json2\n",
      "  json2_24: json2\n",
      "  json2_25: json2\n",
      "  json2_26: json2\n",
      "File attributes after processing:\n",
      "  json1_1: json1 - Original Number: 1 - Encode ID: 10\n",
      "  json1_2: json1 - Original Number: 2 - Encode ID: 8\n",
      "  json1_3: json1 - Original Number: 3 - Encode ID: 2\n",
      "  json1_4: json1 - Original Number: 4 - Encode ID: 3\n",
      "  json1_5: json1 - Original Number: 5 - Encode ID: 9\n",
      "  json1_6: json1 - Original Number: 6 - Encode ID: 1\n",
      "  json1_7: json1 - Original Number: 7 - Encode ID: 13\n",
      "  json1_8: json1 - Original Number: 8 - Encode ID: 11\n",
      "  json1_9: json1 - Original Number: 9 - Encode ID: 4\n",
      "  json1_10: json1 - Original Number: 10 - Encode ID: 5\n",
      "  json1_11: json1 - Original Number: 11 - Encode ID: 15\n",
      "  json1_12: json1 - Original Number: 12 - Encode ID: 14\n",
      "  json1_13: json1 - Original Number: 13 - Encode ID: 6\n",
      "  json1_14: json1 - Original Number: 14 - Encode ID: 12\n",
      "  json1_15: json1 - Original Number: 15 - Encode ID: 7\n",
      "  json2_1: json2 - Original Number: 1 - Encode ID: 10\n",
      "  json2_2: json2 - Original Number: 2 - Encode ID: 8\n",
      "  json2_3: json2 - Original Number: 3 - Encode ID: 2\n",
      "  json2_4: json2 - Original Number: 4 - Encode ID: 3\n",
      "  json2_5: json2 - Original Number: 5 - Encode ID: 9\n",
      "  json2_6: json2 - Original Number: 6 - Encode ID: 1\n",
      "  json2_7: json2 - Original Number: 7 - Encode ID: 13\n",
      "  json2_8: json2 - Original Number: 8 - Encode ID: 11\n",
      "  json2_9: json2 - Original Number: 9 - Encode ID: 4\n",
      "  json2_10: json2 - Original Number: 10 - Encode ID: 5\n",
      "  json2_11: json2 - Original Number: 11 - Encode ID: 15\n",
      "  json2_12: json2 - Original Number: 12 - Encode ID: 14\n",
      "  json2_13: json2 - Original Number: 13 - Encode ID: 6\n",
      "  json2_14: json2 - Original Number: 14 - Encode ID: 12\n",
      "  json2_15: json2 - Original Number: 15 - Encode ID: 7\n",
      "  json2_16: json2 - Original Number: 16 - Encode ID: 10000\n",
      "  json2_17: json2 - Original Number: 17 - Encode ID: 10001\n",
      "  json2_18: json2 - Original Number: 18 - Encode ID: 10002\n",
      "  json2_19: json2 - Original Number: 19 - Encode ID: 10003\n",
      "  json2_20: json2 - Original Number: 20 - Encode ID: 10004\n",
      "  json2_21: json2 - Original Number: 21 - Encode ID: 10005\n",
      "  json2_22: json2 - Original Number: 22 - Encode ID: 10006\n",
      "  json2_23: json2 - Original Number: 23 - Encode ID: 10007\n",
      "  json2_24: json2 - Original Number: 24 - Encode ID: 10008\n",
      "  json2_25: json2 - Original Number: 25 - Encode ID: 10009\n",
      "  json2_26: json2 - Original Number: 26 - Encode ID: 10010\n",
      "Comparison matrix saved to 'output_files/comparison_matrix.xlsx'.\n",
      "NetworkX style output saved to 'output_files/networkx_style_output.txt'.\n",
      "Node Distance (Simple): 11\n",
      "Node Distance (Jaccard): 0.42\n",
      "Key Distance (Simple): 10\n",
      "Key Distance (Jaccard): 0.57\n",
      "Value Distance (Simple): 19\n",
      "Value Distance (Jaccard): 0.85\n",
      "Graph Distance: 11.00\n",
      "Keys in json_objects1: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15']\n",
      "Keys in json_objects2: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'labels1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 774\u001b[0m\n\u001b[0;32m    771\u001b[0m     visualize_graph(nx\u001b[38;5;241m.\u001b[39mcompose(G1, G2))\n\u001b[0;32m    773\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 774\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[7], line 761\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;66;03m# Save Jaccard distances\u001b[39;00m\n\u001b[0;32m    760\u001b[0m sub_similarity_matrix \u001b[38;5;241m=\u001b[39m similarity_matrix[:\u001b[38;5;28mlen\u001b[39m(json_objects1), \u001b[38;5;28mlen\u001b[39m(json_objects1):\u001b[38;5;28mlen\u001b[39m(json_objects1)\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mlen\u001b[39m(json_objects2)]\n\u001b[1;32m--> 761\u001b[0m save_jaccard_distances_to_excel(sub_similarity_matrix, \n\u001b[0;32m    762\u001b[0m     json_objects1, json_objects2, \n\u001b[0;32m    763\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjaccard_distances.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m), G1, G2)\n\u001b[0;32m    765\u001b[0m \u001b[38;5;66;03m# Calculate and save detailed distances\u001b[39;00m\n\u001b[0;32m    766\u001b[0m detailed_distances \u001b[38;5;241m=\u001b[39m calculate_detailed_distances(json_objects1, json_objects2, threshold)\n",
      "Cell \u001b[1;32mIn[7], line 513\u001b[0m, in \u001b[0;36msave_jaccard_distances_to_excel\u001b[1;34m(similarity_matrix, json_objects1, json_objects2, file_path, G1, G2)\u001b[0m\n\u001b[0;32m    505\u001b[0m graph_summary \u001b[38;5;241m=\u001b[39m jaccard_summary(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mset\u001b[39m(G1\u001b[38;5;241m.\u001b[39medges()),\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28mset\u001b[39m(G2\u001b[38;5;241m.\u001b[39medges()),\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGraph Distance\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    509\u001b[0m )\n\u001b[0;32m    511\u001b[0m summary_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([node_summary, key_summary, value_summary, graph_summary])\n\u001b[1;32m--> 513\u001b[0m cosine_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(similarity_submatrix, index\u001b[38;5;241m=\u001b[39mlabels1, columns\u001b[38;5;241m=\u001b[39mlabels2)\n\u001b[0;32m    515\u001b[0m nodes_details \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    516\u001b[0m edges_details \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels1' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import os\n",
    "import heapq\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create a new directory to save all files\n",
    "output_dir = 'output_files'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define global threshold variable\n",
    "threshold = 0.8\n",
    "\n",
    "def flatten_json(y):\n",
    "    out = {}\n",
    "\n",
    "    def flatten(x, name=''):\n",
    "        if type(x) is dict:\n",
    "            for a in x:\n",
    "                flatten(x[a], name + a + '.')\n",
    "        elif type(x) is list:\n",
    "            i = 0\n",
    "            for a in x:\n",
    "                flatten(a, name + str(i) + '.')\n",
    "                i += 1\n",
    "        else:\n",
    "            out[name[:-1]] = x\n",
    "\n",
    "    flatten(y)\n",
    "    return out\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text by lowercasing and removing punctuation.\"\"\"\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "def get_embeddings(text):\n",
    "    \"\"\"Get BERT embeddings for the given text.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "def save_tokenized_texts_combined(tokenized_texts, file_label):\n",
    "    \"\"\"Save combined tokenized texts to a file.\"\"\"\n",
    "    with open(os.path.join(output_dir, f\"combined_tokenized_output_{file_label}.txt\"), \"w\") as f:\n",
    "        for tokenized_text in tokenized_texts:\n",
    "            f.write(\" \".join(tokenized_text) + \"\\n\")\n",
    "\n",
    "def save_embeddings_combined(embeddings, file_label):\n",
    "    \"\"\"Save combined embeddings as .npy file.\"\"\"\n",
    "    combined_embeddings = np.vstack(embeddings)\n",
    "    np.save(os.path.join(output_dir, f\"combined_embeddings_{file_label}.npy\"), combined_embeddings)\n",
    "\n",
    "def json_to_text(data, file_label):\n",
    "    texts = []\n",
    "    embeddings = []\n",
    "    tokenized_texts = []\n",
    "    json_objects = {}\n",
    "    original_ids = {}\n",
    "    object_order = []\n",
    "    for i, entry in enumerate(data):\n",
    "        flattened_entry = flatten_json(entry)\n",
    "        original_id = flattened_entry.pop('id', None)\n",
    "        if original_id:\n",
    "            original_ids[original_id] = flattened_entry\n",
    "\n",
    "        flattened_entry['file'] = file_label\n",
    "        flattened_entry['node_or_edge'] = 'edge' if 'source_ref' in flattened_entry and 'target_ref' in flattened_entry else 'node'\n",
    "        \n",
    "        text = \". \".join(f\"{key}: {value}\" for key, value in flattened_entry.items())\n",
    "        text = preprocess_text(text)\n",
    "        texts.append(text)\n",
    "\n",
    "        tokenized_text = tokenizer.tokenize(text)\n",
    "        tokenized_texts.append(tokenized_text)\n",
    "\n",
    "        embedding = get_embeddings(text)\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "        encode_id = flattened_entry.get('encodeID', i+1)\n",
    "        json_objects[encode_id] = flattened_entry\n",
    "        object_order.append(encode_id)\n",
    "        \n",
    "        print(f\"Processing object {i+1} for {file_label}:\")\n",
    "        print(f\"  Encode ID: {encode_id}\")\n",
    "        print(f\"  Type: {flattened_entry.get('type', 'No type')}\")\n",
    "        print(f\"  Name: {flattened_entry.get('name', 'No name')}\")\n",
    "        print(f\"  Node or Edge: {flattened_entry['node_or_edge']}\")\n",
    "        print(f\"  File: {flattened_entry['file']}\")\n",
    "    \n",
    "    print(f\"Number of objects in {file_label}: {len(json_objects)}\")\n",
    "    print(f\"Number of embeddings in {file_label}: {len(embeddings)}\")\n",
    "    print(f\"Object order in {file_label}: {object_order}\")\n",
    "    \n",
    "    return texts, embeddings, tokenized_texts, json_objects, original_ids, object_order\n",
    "\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"Load JSON data from a file with detailed error handling.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.read()\n",
    "            return json.loads(data)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        return None\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: Failed to decode JSON from file - {file_path}\")\n",
    "        print(f\"Error details: {e}\")\n",
    "        print(f\"Faulty JSON content:\\n{data}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def assign_encode_ids(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, \n",
    "                      original_ids1, original_ids2, threshold, order1, order2):\n",
    "    print(\"Contents of json_objects1:\")\n",
    "    for k, v in json_objects1.items():\n",
    "        print(f\"  {k}: {v.get('type', 'No type')} - {v.get('name', 'No name')} - {v.get('file', 'No file')}\")\n",
    "    \n",
    "    print(\"Contents of json_objects2:\")\n",
    "    for k, v in json_objects2.items():\n",
    "        print(f\"  {k}: {v.get('type', 'No type')} - {v.get('name', 'No name')} - {v.get('file', 'No file')}\")\n",
    "\n",
    "    combined_texts = texts1 + texts2\n",
    "    combined_embeddings = np.vstack(embeddings1 + embeddings2)\n",
    "    \n",
    "    combined_json_objects = {}\n",
    "    for i, (k, v) in enumerate(json_objects1.items(), start=1):\n",
    "        combined_json_objects[f\"json1_{k}\"] = v.copy()\n",
    "        combined_json_objects[f\"json1_{k}\"][\"original_number\"] = i\n",
    "    for i, (k, v) in enumerate(json_objects2.items(), start=1):\n",
    "        combined_json_objects[f\"json2_{k}\"] = v.copy()\n",
    "        combined_json_objects[f\"json2_{k}\"][\"original_number\"] = i\n",
    "    \n",
    "    combined_order = order1 + order2\n",
    "\n",
    "    print(\"File attributes before processing:\")\n",
    "    for k, v in combined_json_objects.items():\n",
    "        print(f\"  {k}: {v.get('file', 'No file attribute')}\")\n",
    "\n",
    "    similarity_matrix = cosine_similarity(combined_embeddings)\n",
    "    num_texts = len(combined_texts)\n",
    "    comparison_matrix = np.zeros((num_texts, num_texts))\n",
    "\n",
    "    # Priority queue to store similarities\n",
    "    similarity_queue = []\n",
    "    \n",
    "    # Calculate similarities and add to priority queue\n",
    "    for i, (key_i, obj_i) in enumerate(combined_json_objects.items()):\n",
    "        for j, (key_j, obj_j) in enumerate(combined_json_objects.items()):\n",
    "            if i < len(texts1) and j >= len(texts1):  # Only compare objects from different files\n",
    "                similarity = similarity_matrix[i, j - len(texts1)]\n",
    "                comparison_matrix[i, j - len(texts1)] = similarity\n",
    "                if similarity > threshold and obj_i['file'] != obj_j['file']:\n",
    "                    heapq.heappush(similarity_queue, (-similarity, key_i, key_j))  # Use negative similarity for max-heap\n",
    "\n",
    "    encode_id = 1\n",
    "    unmatched_encode_id = 10000\n",
    "    encode_id_mapping = {}\n",
    "    assigned_objects = set()\n",
    "\n",
    "    # Process similarities in descending order\n",
    "    while similarity_queue:\n",
    "        similarity, key_i, key_j = heapq.heappop(similarity_queue)\n",
    "        similarity = -similarity  # Convert back to positive\n",
    "        \n",
    "        if key_i not in assigned_objects and key_j not in assigned_objects:\n",
    "            encode_id_mapping[key_i] = encode_id\n",
    "            encode_id_mapping[key_j] = encode_id\n",
    "            assigned_objects.add(key_i)\n",
    "            assigned_objects.add(key_j)\n",
    "            encode_id += 1\n",
    "\n",
    "    # Assign unmatched IDs\n",
    "    for key in combined_json_objects.keys():\n",
    "        if key not in encode_id_mapping:\n",
    "            encode_id_mapping[key] = unmatched_encode_id\n",
    "            unmatched_encode_id += 1\n",
    "\n",
    "    # Update objects with new encode IDs\n",
    "    for key, obj in combined_json_objects.items():\n",
    "        obj['encodeID'] = encode_id_mapping[key]\n",
    "        if 'node_or_edge' not in obj:\n",
    "            obj['node_or_edge'] = 'edge' if 'source_ref' in obj and 'target_ref' in obj else 'node'\n",
    "\n",
    "    # Create a dictionary to map original IDs to encodeIDs\n",
    "    id_to_encodeID = {original_id: obj['encodeID'] for original_id, obj in original_ids1.items() if 'encodeID' in obj}\n",
    "    id_to_encodeID.update({original_id: obj['encodeID'] for original_id, obj in original_ids2.items() if 'encodeID' in obj})\n",
    "\n",
    "    # Update source_ref and target_ref in relationship objects\n",
    "    for obj in combined_json_objects.values():\n",
    "        if obj['node_or_edge'] == 'edge':\n",
    "            obj['source_ref'] = id_to_encodeID.get(obj['source_ref'], obj['source_ref'])\n",
    "            obj['target_ref'] = id_to_encodeID.get(obj['target_ref'], obj['target_ref'])\n",
    "\n",
    "    # Separate objects back into json_objects1 and json_objects2\n",
    "    json_objects1 = {k.split('_')[1]: v for k, v in combined_json_objects.items() if k.startswith('json1_')}\n",
    "    json_objects2 = {k.split('_')[1]: v for k, v in combined_json_objects.items() if k.startswith('json2_')}\n",
    "\n",
    "    print(\"File attributes after processing:\")\n",
    "    for k, v in combined_json_objects.items():\n",
    "        print(f\"  {k}: {v.get('file', 'No file attribute')} - Original Number: {v.get('original_number')} - Encode ID: {v.get('encodeID')}\")\n",
    "\n",
    "    return json_objects1, json_objects2, comparison_matrix, combined_embeddings\n",
    "\n",
    "# Example call to the function (for testing purposes, you should replace with actual data)\n",
    "# assign_encode_ids(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, original_ids1, original_ids2, threshold, order1, order2)\n",
    "\n",
    "\n",
    "def identify_node_or_edge_and_add_key(json_objects):\n",
    "    \"\"\"Identify if an object is a node or an edge based on the presence of two values matching encodeIDs of other objects,\n",
    "    and add a new key-value pair indicating its type.\"\"\"\n",
    "    \n",
    "    encode_ids = set()\n",
    "    for entry in json_objects.values():\n",
    "        if 'encodeID' in entry:\n",
    "            encode_ids.add(entry['encodeID'])\n",
    "        else:\n",
    "            print(f\"Warning: 'encodeID' missing from object: {entry}\")\n",
    "    \n",
    "    for obj in json_objects.values():\n",
    "        if 'encodeID' not in obj:\n",
    "            obj['encodeID'] = max(encode_ids) + 1 if encode_ids else 1\n",
    "            encode_ids.add(obj['encodeID'])\n",
    "        \n",
    "        values = set(obj.values())\n",
    "        if len(values.intersection(encode_ids)) >= 2:\n",
    "            obj['node_or_edge'] = \"edge\"\n",
    "        else:\n",
    "            obj['node_or_edge'] = \"node\"\n",
    "    \n",
    "    return json_objects\n",
    "\n",
    "def save_json(data, file_path):\n",
    "    \"\"\"Save JSON data to a file.\"\"\"\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "def build_graph(json_objects, suffix, color):\n",
    "    \"\"\"Build a graph based on JSON objects.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    valid_nodes = set()\n",
    "\n",
    "    # Add nodes, ensuring unique IDs\n",
    "    for key, obj in json_objects.items():\n",
    "        if obj['node_or_edge'] != 'edge':  # Only add entities as nodes\n",
    "            if obj.get('name') != 'Unknown Name':  # Avoid adding unknown name nodes\n",
    "                unique_id = str(obj.get('encodeID')) + suffix\n",
    "                node_name = f\"{obj.get('encodeID')} ({obj['file']})\"\n",
    "                G.add_node(unique_id, description=obj.get('description', 'No description provided'))\n",
    "                valid_nodes.add(unique_id)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Add edges, using unique IDs\n",
    "    unique_edges = set()\n",
    "    for obj in json_objects.values():\n",
    "        if obj['node_or_edge'] == 'edge':\n",
    "            source = str(obj.get('source_ref')) + suffix\n",
    "            target = str(obj.get('target_ref')) + suffix\n",
    "            edge = (source, target)\n",
    "            if source in valid_nodes and target in valid_nodes and source != target:  # Ensure both nodes are valid and not the same\n",
    "                G.add_edge(source, target)\n",
    "                unique_edges.add(edge)\n",
    "    return G\n",
    "\n",
    "\n",
    "def visualize_graph(G):\n",
    "    \"\"\"Visualize a networkx graph with enhanced label formatting for readability.\"\"\"\n",
    "    num_nodes = len(G.nodes)\n",
    "    figsize = get_optimal_figsize(num_nodes)\n",
    "    k = get_optimal_k(num_nodes)\n",
    "\n",
    "    plt.figure(figsize=figsize)  # Dynamically set figure size\n",
    "    pos = nx.spring_layout(G, k=k)  # Dynamically set layout parameter\n",
    "\n",
    "    # Draw nodes with their corresponding colors\n",
    "    node_colors = [data.get('color', 'grey') for node, data in G.nodes(data=True)]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=300)\n",
    "\n",
    "    # Draw edges with specified styles and colors\n",
    "    for (u, v, data) in G.edges(data=True):\n",
    "        edge_style = data.get('style', 'dotted')\n",
    "        edge_color = data.get('color', 'black')\n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos, edgelist=[(u, v)],\n",
    "            style=edge_style,\n",
    "            edge_color=edge_color,\n",
    "            width=2\n",
    "        )\n",
    "\n",
    "    # Draw edge labels\n",
    "    edge_labels = {(u, v): data.get('label', 'similar-to') for u, v, data in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "    # Draw labels for nodes\n",
    "    labels = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        label = f\"{data.get('label', 'Unknown')}\\n({data.get('type', 'Unknown')})\"\n",
    "        description = data.get('description', 'No description provided')\n",
    "        if description:\n",
    "            doc = nlp(description)\n",
    "            verbs = [token.text for token in doc if token.pos_ == 'VERB']\n",
    "            label += f\"\\n{' '.join(verbs[:2])}\"\n",
    "        labels[node] = label\n",
    "\n",
    "    for node, label in labels.items():\n",
    "        x, y = pos[node]\n",
    "        plt.text(x, y, label, fontsize=9, ha='center', va='center',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='white', edgecolor='gray', alpha=0.6))\n",
    "\n",
    "    plt.title('Graph Visualization')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def get_optimal_figsize(num_nodes):\n",
    "    \"\"\"Determine an optimal figure size based on the number of nodes.\"\"\"\n",
    "    base_size = 10  # Base size for the figure\n",
    "    scale_factor = 0.5  # Scale factor to adjust size\n",
    "    return (base_size + num_nodes * scale_factor, base_size + num_nodes * scale_factor)\n",
    "\n",
    "def get_optimal_k(num_nodes):\n",
    "    \"\"\"Determine an optimal k value for spring_layout based on the number of nodes.\"\"\"\n",
    "    base_k = 0.5  # Base k value\n",
    "    scale_factor = 0.1  # Scale factor to adjust k\n",
    "    return base_k + num_nodes * scale_factor\n",
    "\n",
    "# Define node substitution cost function\n",
    "def node_subst_cost(n1, n2):\n",
    "    \"\"\"Define cost for node substitution.\"\"\"\n",
    "    return 0 if n1 == n2 else 1\n",
    "\n",
    "# Define edge substitution cost function\n",
    "def edge_subst_cost(e1, e2):\n",
    "    \"\"\"Define cost for edge substitution.\"\"\"\n",
    "    return 0 if e1 == e2 else 1\n",
    "\n",
    "def calculate_distances(json_objects1, json_objects2):\n",
    "    \"\"\"Calculate node distance, key distance, value distance, and graph distance.\"\"\"\n",
    "    def jaccard_distance(set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return 1 - intersection / union if union != 0 else 1\n",
    "\n",
    "    def set_distance(set1, set2):\n",
    "        return abs(len(set1) - len(set2))\n",
    "\n",
    "    # Node distance\n",
    "    nodes1 = set(json_objects1.keys())\n",
    "    nodes2 = set(json_objects2.keys())\n",
    "    node_distance = set_distance(nodes1, nodes2)\n",
    "    node_jaccard = jaccard_distance(nodes1, nodes2)\n",
    "\n",
    "    # Key distance\n",
    "    keys1 = set(k for obj in json_objects1.values() for k in obj.keys())\n",
    "    keys2 = set(k for obj in json_objects2.values() for k in obj.keys())\n",
    "    key_distance = set_distance(keys1, keys2)\n",
    "    key_jaccard = jaccard_distance(keys1, keys2)\n",
    "\n",
    "    # Value distance\n",
    "    values1 = set(v for obj in json_objects1.values() for v in obj.values() if isinstance(v, str))\n",
    "    values2 = set(v for obj in json_objects2.values() for v in obj.values() if isinstance(v, str))\n",
    "    value_distance = set_distance(values1, values2)\n",
    "    value_jaccard = jaccard_distance(values1, values2)\n",
    "\n",
    "    # Graph distance (using node and edge comparison)\n",
    "    G1 = build_graph(json_objects1, '_blue', 'blue')\n",
    "    G2 = build_graph(json_objects2, '_red', 'red')\n",
    "    graph_distance = nx.graph_edit_distance(G1, G2, node_subst_cost=node_subst_cost, edge_subst_cost=edge_subst_cost)\n",
    "\n",
    "    print(f\"Node Distance (Simple): {node_distance}\")\n",
    "    print(f\"Node Distance (Jaccard): {node_jaccard:.2f}\")\n",
    "    print(f\"Key Distance (Simple): {key_distance}\")\n",
    "    print(f\"Key Distance (Jaccard): {key_jaccard:.2f}\")\n",
    "    print(f\"Value Distance (Simple): {value_distance}\")\n",
    "    print(f\"Value Distance (Jaccard): {value_jaccard:.2f}\")\n",
    "    print(f\"Graph Distance: {graph_distance:.2f}\")\n",
    "\n",
    "def create_comparison_matrix(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, threshold):\n",
    "    combined_embeddings = np.vstack(embeddings1 + embeddings2)\n",
    "    similarity_matrix = cosine_similarity(combined_embeddings)\n",
    "\n",
    "    num_texts1 = len(texts1)\n",
    "    num_texts2 = len(texts2)\n",
    "    comparison_data = []\n",
    "\n",
    "    for i in range(num_texts1):\n",
    "        row = []\n",
    "        for j in range(num_texts2):\n",
    "            similarity = similarity_matrix[i, num_texts1 + j]\n",
    "            match = 1 if similarity > threshold else 0\n",
    "            row.append(match)\n",
    "        comparison_data.append(row)\n",
    "\n",
    "    labels1 = [f\"{obj['original_number']}-{obj['encodeID']}\" for obj in json_objects1.values()]\n",
    "    labels2 = [f\"{obj['original_number']}-{obj['encodeID']}\" for obj in json_objects2.values()]\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data, index=labels1, columns=labels2)\n",
    "\n",
    "    return comparison_df, similarity_matrix\n",
    "\n",
    "\n",
    "def save_comparison_matrix_to_excel(comparison_df, similarity_matrix, file_path, labels1, labels2):\n",
    "    with pd.ExcelWriter(os.path.join(output_dir, file_path)) as writer:\n",
    "        comparison_df.to_excel(writer, sheet_name='Comparison Matrix')\n",
    "\n",
    "        similarity_submatrix = similarity_matrix[:len(labels1), len(labels1):len(labels1)+len(labels2)]\n",
    "        similarity_df = pd.DataFrame(similarity_submatrix, index=labels1, columns=labels2)\n",
    "        similarity_df.to_excel(writer, sheet_name='Similarity Values')\n",
    "\n",
    "def save_embeddings_to_excel(embeddings1, embeddings2, json_objects1, json_objects2, file_path, threshold):\n",
    "    aggregated_embeddings1 = np.array([embedding.mean(axis=0) for embedding in embeddings1])\n",
    "    aggregated_embeddings2 = np.array([embedding.mean(axis=0) for embedding in embeddings2])\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(aggregated_embeddings1, aggregated_embeddings2)\n",
    "    \n",
    "    labels1 = [f\"{obj['original_number']}-{obj['encodeID']}\" for obj in json_objects1.values()]\n",
    "    labels2 = [f\"{obj['original_number']}-{obj['encodeID']}\" for obj in json_objects2.values()]\n",
    "    \n",
    "    similarity_df = pd.DataFrame(similarity_matrix, index=labels1, columns=labels2)\n",
    "    \n",
    "    match_matrix = (similarity_matrix > threshold).astype(int)\n",
    "    match_df = pd.DataFrame(match_matrix, index=labels1, columns=labels2)\n",
    "    \n",
    "    with pd.ExcelWriter(os.path.join(output_dir, file_path)) as writer:\n",
    "        similarity_df.to_excel(writer, sheet_name='Similarity Matrix')\n",
    "        match_df.to_excel(writer, sheet_name='Match Matrix')\n",
    "\n",
    "def save_jaccard_distances_to_excel(similarity_matrix, json_objects1, json_objects2, file_path, G1, G2):\n",
    "    num_texts1 = len(json_objects1)\n",
    "    num_texts2 = len(json_objects2)\n",
    "    jaccard_distances = []\n",
    "\n",
    "    similarity_submatrix = similarity_matrix[:num_texts1, :num_texts2]\n",
    "\n",
    "    if similarity_submatrix.shape != (num_texts1, num_texts2):\n",
    "        raise ValueError(f\"Expected submatrix shape ({num_texts1}, {num_texts2}), but got {similarity_submatrix.shape}\")\n",
    "\n",
    "    print(f\"Keys in json_objects1: {list(json_objects1.keys())}\")\n",
    "    print(f\"Keys in json_objects2: {list(json_objects2.keys())}\")\n",
    "\n",
    "    keys1 = list(json_objects1.keys())\n",
    "    keys2 = list(json_objects2.keys())\n",
    "\n",
    "    for i, key1 in enumerate(keys1):\n",
    "        for j, key2 in enumerate(keys2):\n",
    "            intersection = similarity_submatrix[i, j]\n",
    "            union = 1\n",
    "            jaccard_distance = 1 - intersection / union if union != 0 else 1\n",
    "\n",
    "            obj1 = json_objects1[key1]\n",
    "            obj2 = json_objects2[key2]\n",
    "\n",
    "            type1 = obj1.get('node_or_edge', 'unknown')\n",
    "            type2 = obj2.get('node_or_edge', 'unknown')\n",
    "\n",
    "            jaccard_distances.append({\n",
    "                'Text 1 Index': i,\n",
    "                'Text 2 Index': j,\n",
    "                'Text 1 Label': f\"{obj1['original_number']}-{obj1['encodeID']}\",\n",
    "                'Text 2 Label': f\"{obj2['original_number']}-{obj2['encodeID']}\",\n",
    "                'Type 1': type1,\n",
    "                'Type 2': type2,\n",
    "                'Intersection (Cosine Similarity)': intersection,\n",
    "                'Union': union,\n",
    "                'Jaccard Distance': jaccard_distance\n",
    "            })\n",
    "\n",
    "    jaccard_df = pd.DataFrame(jaccard_distances)\n",
    "\n",
    "    node_summary = jaccard_summary(\n",
    "        set(obj['encodeID'] for obj in json_objects1.values()),\n",
    "        set(obj['encodeID'] for obj in json_objects2.values()),\n",
    "        'Node Distance'\n",
    "    )\n",
    "\n",
    "    key_summary = jaccard_summary(\n",
    "        set(k for obj in json_objects1.values() for k in obj.keys()),\n",
    "        set(k for obj in json_objects2.values() for k in obj.keys()),\n",
    "        'Key Distance'\n",
    "    )\n",
    "\n",
    "    value_summary = jaccard_summary(\n",
    "        set(v for obj in json_objects1.values() for v in obj.values() if isinstance(v, str)),\n",
    "        set(v for obj in json_objects2.values() for v in obj.values() if isinstance(v, str)),\n",
    "        'Value Distance'\n",
    "    )\n",
    "\n",
    "    graph_summary = jaccard_summary(\n",
    "        set(G1.edges()),\n",
    "        set(G2.edges()),\n",
    "        'Graph Distance'\n",
    "    )\n",
    "\n",
    "    summary_df = pd.DataFrame([node_summary, key_summary, value_summary, graph_summary])\n",
    "\n",
    "    cosine_df = pd.DataFrame(similarity_submatrix, index=labels1, columns=labels2)\n",
    "\n",
    "    nodes_details = []\n",
    "    edges_details = []\n",
    "\n",
    "    for i in range(num_texts1):\n",
    "        for j in range(num_texts2):\n",
    "            type1 = json_objects1[labels1[i]]['node_or_edge']\n",
    "            type2 = json_objects2[labels2[j]]['node_or_edge']\n",
    "            if type1 == 'node' and type2 == 'node':\n",
    "                nodes_details.append({\n",
    "                    'Text 1 Index': i,\n",
    "                    'Text 2 Index': j,\n",
    "                    'Text 1 Label': labels1[i],\n",
    "                    'Text 2 Label': labels2[j],\n",
    "                    'Cosine Similarity': similarity_submatrix[i, j]\n",
    "                })\n",
    "            elif type1 == 'edge' and type2 == 'edge':\n",
    "                edges_details.append({\n",
    "                    'Text 1 Index': i,\n",
    "                    'Text 2 Index': j,\n",
    "                    'Text 1 Label': labels1[i],\n",
    "                    'Text 2 Label': labels2[j],\n",
    "                    'Cosine Similarity': similarity_submatrix[i, j]\n",
    "                })\n",
    "\n",
    "    nodes_df = pd.DataFrame(nodes_details)\n",
    "    edges_df = pd.DataFrame(edges_details)\n",
    "\n",
    "    with pd.ExcelWriter(file_path) as writer:\n",
    "        cosine_df.to_excel(writer, sheet_name='Cosine Similarity')\n",
    "        nodes_df.to_excel(writer, sheet_name='Nodes Details', index=False)\n",
    "        edges_df.to_excel(writer, sheet_name='Edges Details', index=False)\n",
    "        jaccard_df.to_excel(writer, sheet_name='Jaccard Distances', index=False)\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "    print(f\"Jaccard distances saved to '{file_path}'.\")\n",
    "\n",
    "def jaccard_summary(set1, set2, label):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    jaccard_distance = 1 - intersection / union if union != 0 else 1\n",
    "    return {\n",
    "        'Metric': f'{label} (Jaccard)',\n",
    "        'Distance': jaccard_distance,\n",
    "        'Intersection': intersection,\n",
    "        'Union': union\n",
    "    }\n",
    "\n",
    "def calculate_detailed_distances(json_objects1, json_objects2, threshold):\n",
    "    node_i, node_d, node_s = 0, 0, 0\n",
    "    edge_i, edge_d, edge_s = 0, 0, 0\n",
    "    key_i, key_d, key_s = 0, 0, 0\n",
    "    value_i, value_d, value_s = 0, 0, 0\n",
    "\n",
    "    # Identify nodes and edges\n",
    "    nodes1 = {k: v for k, v in json_objects1.items() if v['node_or_edge'] == 'node'}\n",
    "    nodes2 = {k: v for k, v in json_objects2.items() if v['node_or_edge'] == 'node'}\n",
    "    edges1 = {k: v for k, v in json_objects1.items() if v['node_or_edge'] == 'edge'}\n",
    "    edges2 = {k: v for k, v in json_objects2.items() if v['node_or_edge'] == 'edge'}\n",
    "\n",
    "    # Debug: Print the nodes and edges\n",
    "    print(\"Nodes in JSON1:\", list(nodes1.keys()))\n",
    "    print(\"Nodes in JSON2:\", list(nodes2.keys()))\n",
    "    print(\"Edges in JSON1:\", list(edges1.keys()))\n",
    "    print(\"Edges in JSON2:\", list(edges2.keys()))\n",
    "\n",
    "    # Compare nodes\n",
    "    nodes_only_in_1 = set(nodes1.keys()) - set(nodes2.keys())\n",
    "    nodes_only_in_2 = set(nodes2.keys()) - set(nodes1.keys())\n",
    "    node_s = min(len(nodes_only_in_1), len(nodes_only_in_2))\n",
    "    node_d = len(nodes_only_in_1) - node_s\n",
    "    node_i = len(nodes_only_in_2) - node_s\n",
    "\n",
    "    print(f\"Node substitutions: {node_s}\")\n",
    "    print(f\"Node deletions: {node_d}\")\n",
    "    print(f\"Node insertions: {node_i}\")\n",
    "\n",
    "    # Compare edges based on their content, not just IDs\n",
    "    edges1_set = {(e['source_ref'], e['target_ref']) for e in edges1.values()}\n",
    "    edges2_set = {(e['source_ref'], e['target_ref']) for e in edges2.values()}\n",
    "    \n",
    "    edges_only_in_1 = edges1_set - edges2_set\n",
    "    edges_only_in_2 = edges2_set - edges1_set\n",
    "    \n",
    "    edge_d = len(edges_only_in_1)\n",
    "    edge_i = len(edges_only_in_2)\n",
    "    edge_s = 0  # There are no substitutions for edges, only insertions and deletions\n",
    "\n",
    "    print(f\"Edge substitutions: {edge_s}\")\n",
    "    print(f\"Edge deletions: {edge_d}\")\n",
    "    print(f\"Edge insertions: {edge_i}\")\n",
    "\n",
    "    # Compare keys and values for nodes present in both JSONs\n",
    "    for node_id in set(nodes1.keys()) & set(nodes2.keys()):\n",
    "        obj1, obj2 = nodes1[node_id], nodes2[node_id]\n",
    "        keys1, keys2 = set(obj1.keys()) - {'file'}, set(obj2.keys()) - {'file'}\n",
    "        \n",
    "        keys_only_in_1 = keys1 - keys2\n",
    "        keys_only_in_2 = keys2 - keys1\n",
    "        key_s += min(len(keys_only_in_1), len(keys_only_in_2))\n",
    "        key_d += len(keys_only_in_1) - key_s\n",
    "        key_i += len(keys_only_in_2) - key_s\n",
    "\n",
    "        for key in keys1 & keys2:\n",
    "            if key != 'file' and obj1[key] != obj2[key]:\n",
    "                value_s += 1\n",
    "\n",
    "    print(f\"Key substitutions: {key_s}\")\n",
    "    print(f\"Key deletions: {key_d}\")\n",
    "    print(f\"Key insertions: {key_i}\")\n",
    "    print(f\"Value substitutions: {value_s}\")\n",
    "\n",
    "    # Calculate distances\n",
    "    node_distance = node_i + node_d + node_s\n",
    "    edge_distance = edge_i + edge_d + edge_s\n",
    "    key_distance = key_i + key_d + key_s\n",
    "    value_distance = value_s  # value_i and value_d are not used in this logic\n",
    "\n",
    "    detailed_distance = [{\n",
    "        \"node_distance\": node_distance,\n",
    "        \"node_i\": node_i,\n",
    "        \"node_d\": node_d,\n",
    "        \"node_s\": node_s,\n",
    "        \"edge_distance\": edge_distance,\n",
    "        \"edge_i\": edge_i,\n",
    "        \"edge_d\": edge_d,\n",
    "        \"edge_s\": edge_s,\n",
    "        \"key_distance\": key_distance,\n",
    "        \"key_i\": key_i,\n",
    "        \"key_d\": key_d,\n",
    "        \"key_s\": key_s,\n",
    "        \"value_distance\": value_distance,\n",
    "        \"value_i\": 0,\n",
    "        \"value_d\": 0,\n",
    "        \"value_s\": value_s\n",
    "    }]\n",
    "\n",
    "    return detailed_distance\n",
    "\n",
    "def calculate_graph_edit_distance(G1, G2):\n",
    "    def node_subst_cost(n1, n2):\n",
    "        return 0 if n1 == n2 else 1\n",
    "\n",
    "    def node_del_cost(n):\n",
    "        return 1\n",
    "\n",
    "    def node_ins_cost(n):\n",
    "        return 1\n",
    "\n",
    "    def edge_subst_cost(e1, e2):\n",
    "        return 0 if e1 == e2 else 1\n",
    "\n",
    "    def edge_del_cost(e):\n",
    "        return 1\n",
    "\n",
    "    def edge_ins_cost(e):\n",
    "        return 1\n",
    "\n",
    "    ged = nx.graph_edit_distance(\n",
    "        G1, G2,\n",
    "        node_subst_cost=node_subst_cost,\n",
    "        node_del_cost=node_del_cost,\n",
    "        node_ins_cost=node_ins_cost,\n",
    "        edge_subst_cost=edge_subst_cost,\n",
    "        edge_del_cost=edge_del_cost,\n",
    "        edge_ins_cost=edge_ins_cost\n",
    "    )\n",
    "\n",
    "    return ged\n",
    "\n",
    "def output_networkx_style(G1, G2):\n",
    "    ged = calculate_graph_edit_distance(G1, G2)\n",
    "    \n",
    "    output = f\"Graph Edit Distance: {ged}\\n\"\n",
    "    output += f\"Inserted Nodes: {set(G2.nodes()) - set(G1.nodes())}\\n\"\n",
    "    output += f\"Deleted Nodes: {set(G1.nodes()) - set(G2.nodes())}\\n\"\n",
    "    output += f\"Substituted Nodes: {set(G1.nodes()) & set(G2.nodes())}\\n\"\n",
    "    output += f\"Inserted Edges: {set(G2.edges()) - set(G1.edges())}\\n\"\n",
    "    output += f\"Deleted Edges: {set(G1.edges()) - set(G2.edges())}\\n\"\n",
    "    output += f\"Substituted Edges: {set(G1.edges()) & set(G2.edges())}\\n\"\n",
    "\n",
    "    with open(os.path.join(output_dir, 'networkx_style_output.txt'), 'w') as f:\n",
    "        f.write(output)\n",
    "\n",
    "    print(\"NetworkX style output saved to 'output_files/networkx_style_output.txt'.\")\n",
    "\n",
    "def main():\n",
    "    global threshold\n",
    "    # Get input from user\n",
    "    path1 = input(\"Enter the file path for JSON 1: \").strip('\"')\n",
    "    path2 = input(\"Enter the file path for JSON 2: \").strip('\"')\n",
    "    threshold = float(input(\"Enter the similarity threshold (e.g., 0.95): \").strip())\n",
    "\n",
    "    # Load JSON data\n",
    "    data1 = load_json(path1)\n",
    "    data2 = load_json(path2)\n",
    "    if data1 is None or data2 is None:\n",
    "        return\n",
    "\n",
    "    # Process JSON data\n",
    "    texts1, embeddings1, tokenized_texts1, json_objects1, original_ids1, order1 = json_to_text(data1, 'json1')\n",
    "    texts2, embeddings2, tokenized_texts2, json_objects2, original_ids2, order2 = json_to_text(data2, 'json2')\n",
    "\n",
    "    print(f\"Number of objects before assign_encode_ids, json_objects1: {len(json_objects1)}, json_objects2: {len(json_objects2)}\")\n",
    "\n",
    "    # Assign encode IDs\n",
    "    json_objects1, json_objects2, comparison_matrix, combined_embeddings = assign_encode_ids(\n",
    "        texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2,\n",
    "        original_ids1, original_ids2, threshold, order1, order2)\n",
    "\n",
    "    if not json_objects1:\n",
    "        print(\"Error: json_objects1 is empty after processing.\")\n",
    "        return\n",
    "\n",
    "    # Save processed data\n",
    "    combined_tokenized_texts = tokenized_texts1 + tokenized_texts2\n",
    "    save_tokenized_texts_combined(combined_tokenized_texts, 'combined')\n",
    "    save_embeddings_combined(embeddings1 + embeddings2, 'combined')\n",
    "\n",
    "    json_objects1 = identify_node_or_edge_and_add_key(json_objects1)\n",
    "    json_objects2 = identify_node_or_edge_and_add_key(json_objects2)\n",
    "\n",
    "    save_embeddings_to_excel(embeddings1, embeddings2,\n",
    "        json_objects1, json_objects2,\n",
    "        'embeddings_and_similarity.xlsx', threshold)\n",
    "\n",
    "    save_json(json_objects1, os.path.join(output_dir, 'normalized_data1.json'))\n",
    "    save_json(json_objects2, os.path.join(output_dir, 'normalized_data2.json'))\n",
    "\n",
    "    # Create and save comparison matrix\n",
    "    comparison_df, similarity_matrix = create_comparison_matrix(texts1, embeddings1, texts2, embeddings2, json_objects1, json_objects2, threshold)\n",
    "    save_comparison_matrix_to_excel(comparison_df, similarity_matrix, 'comparison_matrix.xlsx',\n",
    "        [f\"{obj['original_number']}-{obj['encodeID']}\" for obj in json_objects1.values()],\n",
    "        [f\"{obj['original_number']}-{obj['encodeID']}\" for obj in json_objects2.values()])\n",
    "\n",
    "    print(\"Comparison matrix saved to 'output_files/comparison_matrix.xlsx'.\")\n",
    "\n",
    "    # Build graphs\n",
    "    G1 = build_graph(json_objects1, '_blue', 'blue')\n",
    "    G2 = build_graph(json_objects2, '_red', 'red')\n",
    "\n",
    "    output_networkx_style(G1, G2)\n",
    "\n",
    "    calculate_distances(json_objects1, json_objects2)\n",
    "\n",
    "    # Save Jaccard distances\n",
    "    sub_similarity_matrix = similarity_matrix[:len(json_objects1), len(json_objects1):len(json_objects1)+len(json_objects2)]\n",
    "    save_jaccard_distances_to_excel(sub_similarity_matrix, \n",
    "        json_objects1, json_objects2, \n",
    "        os.path.join(output_dir, 'jaccard_distances.xlsx'), G1, G2)\n",
    "\n",
    "    # Calculate and save detailed distances\n",
    "    detailed_distances = calculate_detailed_distances(json_objects1, json_objects2, threshold)\n",
    "    save_json(detailed_distances, os.path.join(output_dir, 'detailed_distances.json'))\n",
    "    print(\"Detailed distances saved to 'output_files/detailed_distances.json'.\")\n",
    "\n",
    "    # Visualize graph\n",
    "    visualize_graph(nx.compose(G1, G2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
