{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omar2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omar2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: File not found - \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import silhouette_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load the spaCy NLP model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"Load JSON data from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        return None\n",
    "\n",
    "def json_to_text(data):\n",
    "    \"\"\"Convert JSON entries to text for analysis.\"\"\"\n",
    "    texts = []\n",
    "    ids = []\n",
    "    json_objects = {}\n",
    "    for entry in data:\n",
    "        text = \". \".join(f\"{key}: {value}\" if not isinstance(value, list) else f\"{key}: \" + \", \".join(map(str, value)) for key, value in entry.items())\n",
    "        texts.append(text)\n",
    "        ids.append(entry['id'])\n",
    "        json_objects[entry['id']] = entry\n",
    "    return texts, ids, json_objects\n",
    "\n",
    "def preprocess_json_objects(json_objects):\n",
    "    \"\"\"Ensure all JSON objects have required fields.\"\"\"\n",
    "    for obj in json_objects.values():\n",
    "        obj.setdefault('name', 'Unknown Name')\n",
    "        obj.setdefault('type', 'Unknown Type')\n",
    "        obj.setdefault('description', 'No description provided')\n",
    "    return json_objects\n",
    "\n",
    "def extract_verb_centered_snippet(description, window=2, max_gap=4):\n",
    "    \"\"\"Extract snippets around the first two verbs found in the description.\"\"\"\n",
    "    doc = nlp(description)\n",
    "    verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            verbs.append((token.i, token))\n",
    "            if len(verbs) == 2:\n",
    "                break\n",
    "\n",
    "    if not verbs:\n",
    "        return description\n",
    "\n",
    "    if len(verbs) == 1:\n",
    "        start = max(0, verbs[0][1].i - window)\n",
    "        end = min(len(doc), verbs[0][1].i + window + 1)\n",
    "        return doc[start:end].text\n",
    "\n",
    "    v1, v2 = verbs\n",
    "    if (v2[0] - v1[0]) <= max_gap:\n",
    "        start = max(0, v1[1].i - window)\n",
    "        end = min(len(doc), v2[1].i + window + 1)\n",
    "        return doc[start:end].text\n",
    "    else:\n",
    "        snippet1 = doc[max(0, v1[1].i - window):v1[1].i + window + 1].text\n",
    "        snippet2 = doc[max(0, v2[1].i - window):v2[1].i + window + 1].text\n",
    "        return f\"{snippet1} ... {snippet2}\"\n",
    "\n",
    "def build_graph(json_objects, suffix, color):\n",
    "    \"\"\"Build a graph based on JSON objects.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    valid_nodes = set()\n",
    "\n",
    "    # Add nodes, ensuring unique IDs\n",
    "    for key, obj in json_objects.items():\n",
    "        if obj['type'] != 'relationship':  # Only add entities as nodes\n",
    "            if obj.get('name') != 'Unknown Name':  # Avoid adding unknown name nodes\n",
    "                unique_id = key + suffix\n",
    "                node_name = f\"{obj.get('name', 'Unknown Entity')} ({color})\"\n",
    "                G.add_node(unique_id, label=node_name, color=color,\n",
    "                           description=obj.get('description', 'No description provided'))\n",
    "                valid_nodes.add(unique_id)\n",
    "                print(f\"Added node: {unique_id} with name: {node_name}\")\n",
    "            else:\n",
    "                print(f\"Filtered out node with id: {key} due to 'Unknown Name'\")\n",
    "\n",
    "    # Add edges, using unique IDs\n",
    "    unique_edges = set()\n",
    "    for obj in json_objects.values():\n",
    "        if obj['type'] == 'relationship':\n",
    "            source = obj['source_ref'] + suffix\n",
    "            target = obj['target_ref'] + suffix\n",
    "            edge = (source, target)\n",
    "            if source in valid_nodes and target in valid_nodes and source != target:  # Ensure both nodes are valid and not the same\n",
    "                if edge in unique_edges:\n",
    "                    G[source][target]['style'] = 'solid'\n",
    "                    G[source][target]['color'] = 'black'\n",
    "                else:\n",
    "                    G.add_edge(source, target, style='dotted', color=color, label=obj.get('relationship_type', 'similar-to'))\n",
    "                    unique_edges.add(edge)\n",
    "                print(f\"Added edge: {source} -> {target}\")\n",
    "            else:\n",
    "                print(f\"Filtered out edge from {source} to {target} due to invalid nodes\")\n",
    "\n",
    "    return G\n",
    "\n",
    "def get_optimal_figsize(num_nodes):\n",
    "    \"\"\"Determine an optimal figure size based on the number of nodes.\"\"\"\n",
    "    base_size = 10  # Base size for the figure\n",
    "    scale_factor = 0.5  # Scale factor to adjust size\n",
    "    return (base_size + num_nodes * scale_factor, base_size + num_nodes * scale_factor)\n",
    "\n",
    "def get_optimal_k(num_nodes):\n",
    "    \"\"\"Determine an optimal k value for spring_layout based on the number of nodes.\"\"\"\n",
    "    base_k = 0.5  # Base k value\n",
    "    scale_factor = 0.1  # Scale factor to adjust k\n",
    "    return base_k + num_nodes * scale_factor\n",
    "\n",
    "def visualize_graph(G):\n",
    "    \"\"\"Visualize a networkx graph with enhanced label formatting for readability.\"\"\"\n",
    "    num_nodes = len(G.nodes)\n",
    "    figsize = get_optimal_figsize(num_nodes)\n",
    "    k = get_optimal_k(num_nodes)\n",
    "\n",
    "    plt.figure(figsize=figsize)  # Dynamically set figure size\n",
    "    pos = nx.spring_layout(G, k=k)  # Dynamically set layout parameter\n",
    "\n",
    "    # Draw nodes with their corresponding colors\n",
    "    node_colors = [data.get('color', 'grey') for node, data in G.nodes(data=True)]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=300)\n",
    "\n",
    "    # Draw edges with specified styles and colors\n",
    "    for (u, v, data) in G.edges(data=True):\n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos, edgelist=[(u, v)],\n",
    "            style=data.get('style', 'dotted' if data['color'] == 'gray' else 'solid'),\n",
    "            edge_color=data.get('color', 'black'),\n",
    "            width=2\n",
    "        )\n",
    "\n",
    "    # Draw edge labels\n",
    "    edge_labels = {(u, v): data.get('label', 'similar-to') for u, v, data in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "    # Draw labels for nodes\n",
    "    labels = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        label = f\"{data.get('label', 'Unknown')}\\n({data.get('type', 'Unknown')})\"\n",
    "        description = data.get('description', 'No description provided')\n",
    "        if description:\n",
    "            snippet = extract_verb_centered_snippet(description)\n",
    "            label += f\"\\n{snippet}\"\n",
    "        labels[node] = label\n",
    "\n",
    "    for node, label in labels.items():\n",
    "        x, y = pos[node]\n",
    "        plt.text(x, y, label, fontsize=9, ha='center', va='center',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='white', edgecolor='gray', alpha=0.6))\n",
    "\n",
    "    plt.title('Graph Visualization')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_threshold_analysis(similarity_matrix):\n",
    "    thresholds = np.linspace(0, 1, 50)\n",
    "    num_edges = []\n",
    "    silhouette_scores = []\n",
    "    num_components = []\n",
    "    avg_clustering = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        G = nx.Graph()\n",
    "        labels = np.zeros(len(similarity_matrix))  # Correct the size of the labels array\n",
    "        label_idx = 1\n",
    "        for i, row in enumerate(similarity_matrix):\n",
    "            for j, similarity in enumerate(row):\n",
    "                if similarity > threshold:\n",
    "                    G.add_edge(i, j)\n",
    "                    if labels[i] == 0:\n",
    "                        labels[i] = label_idx\n",
    "                        label_idx += 1\n",
    "                    if j < len(labels):\n",
    "                        labels[j] = labels[i]\n",
    "        num_edges.append(len(G.edges))\n",
    "        num_components.append(nx.number_connected_components(G))\n",
    "        if len(G) > 0:\n",
    "            avg_clustering.append(nx.average_clustering(G))\n",
    "        else:\n",
    "            avg_clustering.append(0)\n",
    "        if len(set(labels)) > 1:\n",
    "            silhouette_scores.append(silhouette_score(similarity_matrix, labels))\n",
    "        else:\n",
    "            silhouette_scores.append(-1)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.plot(thresholds, num_edges, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Number of Edges (Find the Elbow Point)')\n",
    "    plt.title('Elbow Method for Optimal Threshold')\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.plot(thresholds, silhouette_scores, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Silhouette Score (Higher is Better)')\n",
    "    plt.title('Silhouette Analysis for Optimal Threshold')\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.plot(thresholds, num_components, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Number of Components (Stable Plateau)')\n",
    "    plt.title('Number of Connected Components')\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.plot(thresholds, avg_clustering, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Average Clustering Coefficient (Higher is Better)')\n",
    "    plt.title('Average Clustering Coefficient')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def recommend_optimal_threshold(similarity_matrix):\n",
    "    thresholds = np.linspace(0, 1, 50)\n",
    "    num_edges = []\n",
    "    silhouette_scores = []\n",
    "    num_components = []\n",
    "    avg_clustering = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        G = nx.Graph()\n",
    "        labels = np.zeros(len(similarity_matrix))\n",
    "        label_idx = 1\n",
    "        for i, row in enumerate(similarity_matrix):\n",
    "            for j, similarity in enumerate(row):\n",
    "                if similarity > threshold:\n",
    "                    G.add_edge(i, j)\n",
    "                    if labels[i] == 0:\n",
    "                        labels[i] = label_idx\n",
    "                        label_idx += 1\n",
    "                    if j < len(labels):\n",
    "                        labels[j] = labels[i]\n",
    "        num_edges.append(len(G.edges))\n",
    "        num_components.append(nx.number_connected_components(G))\n",
    "        if len(G) > 0:\n",
    "            avg_clustering.append(nx.average_clustering(G))\n",
    "        else:\n",
    "            avg_clustering.append(0)\n",
    "        if len(set(labels)) > 1:\n",
    "            silhouette_scores.append(silhouette_score(similarity_matrix, labels))\n",
    "        else:\n",
    "            silhouette_scores.append(-1)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    num_edges_scaled = scaler.fit_transform(np.array(num_edges).reshape(-1, 1)).flatten()\n",
    "    silhouette_scores_scaled = scaler.fit_transform(np.array(silhouette_scores).reshape(-1, 1)).flatten()\n",
    "    num_components_scaled = scaler.fit_transform(np.array(num_components).reshape(-1, 1)).flatten()\n",
    "    avg_clustering_scaled = scaler.fit_transform(np.array(avg_clustering).reshape(-1, 1)).flatten()\n",
    "\n",
    "    aggregate_score = (num_edges_scaled + silhouette_scores_scaled + (1 - num_components_scaled) + avg_clustering_scaled) / 4\n",
    "    optimal_threshold = thresholds[np.argmax(aggregate_score)]\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(thresholds, aggregate_score, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Aggregated Score')\n",
    "    plt.title('Recommended Threshold Analysis')\n",
    "    plt.axvline(x=optimal_threshold, color='r', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Recommended threshold based on aggregated metrics: {optimal_threshold:.2f}\")\n",
    "\n",
    "def calculate_distances(json_objects1, json_objects2):\n",
    "    \"\"\"Calculate node distance, key distance, value distance, and graph distance.\"\"\"\n",
    "    def jaccard_distance(set1, set2):\n",
    "        intersection = len(set1.intersection(set2))\n",
    "        union = len(set1.union(set2))\n",
    "        return 1 - intersection / union if union != 0 else 1\n",
    "    #Jaccard Distance = 1 minus (number of elements in the intersection of A and B) divided by (number of elements in the union of A and B)\n",
    "\n",
    "    # Node distance\n",
    "    nodes1 = set(json_objects1.keys())\n",
    "    nodes2 = set(json_objects2.keys())\n",
    "    node_distance = jaccard_distance(nodes1, nodes2)\n",
    "\n",
    "    # Key distance\n",
    "    keys1 = set(k for obj in json_objects1.values() for k in obj.keys())\n",
    "    keys2 = set(k for obj in json_objects2.values() for k in obj.keys())\n",
    "    key_distance = jaccard_distance(keys1, keys2)\n",
    "\n",
    "    # Value distance\n",
    "    values1 = set(v for obj in json_objects1.values() for v in obj.values() if isinstance(v, str))\n",
    "    values2 = set(v for obj in json_objects2.values() for v in obj.values() if isinstance(v, str))\n",
    "    value_distance = jaccard_distance(values1, values2)\n",
    "\n",
    "\n",
    "    # Graph distance (using node and edge comparison)\n",
    "    G1 = build_graph(json_objects1, '_blue', 'blue')\n",
    "    G2 = build_graph(json_objects2, '_red', 'red')\n",
    "    graph_distance = nx.graph_edit_distance(G1, G2)\n",
    "\n",
    "    print(f\"Node Distance: {node_distance:.2f}\")\n",
    "    print(f\"Key Distance: {key_distance:.2f}\")\n",
    "    print(f\"Value Distance: {value_distance:.2f}\")\n",
    "    print(f\"Graph Distance: {graph_distance:.2f}\")\n",
    "\n",
    "    '''The equation calculates the Jaccard distance between the sets of string values from two JSON objects. Here’s a step-by-step breakdown:\n",
    "\n",
    "    Extract Values from JSON Objects:\n",
    "\n",
    "    values1 and values2 are sets containing all string values from json_objects1 and json_objects2, respectively.\n",
    "    The comprehension set(v for obj in json_objects1.values() for v in obj.values() if isinstance(v, str)) iterates through all objects (obj) in json_objects1, and then through all values (v) in each object. It includes v in the set if v is a string. The same process applies to json_objects2 to get values2.\n",
    "    Calculate Jaccard Distance:\n",
    "\n",
    "    The function jaccard_distance(values1, values2) calculates the Jaccard distance between the two sets of string values.\n",
    "    The Jaccard distance is a measure of how dissimilar two sets are. It is calculated as:\n",
    "    Jaccard Distance = 1 - (|Intersection of values1 and values2| / |Union of values1 and values2|)\n",
    "    In this formula, |Intersection of values1 and values2| represents the number of elements common to both sets, and |Union of values1 and values2| represents the total number of unique elements in both sets combined.\n",
    "    Example\n",
    "    Suppose json_objects1 has string values {\"a\", \"b\", \"c\"} and json_objects2 has string values {\"b\", \"c\", \"d\"}:\n",
    "\n",
    "    values1 would be {\"a\", \"b\", \"c\"}\n",
    "    values2 would be {\"b\", \"c\", \"d\"}\n",
    "    The Jaccard distance calculation would be:\n",
    "\n",
    "    Intersection: {\"b\", \"c\"} (2 elements)\n",
    "    Union: {\"a\", \"b\", \"c\", \"d\"} (4 elements)\n",
    "    Jaccard Distance = 1 - (2 / 4) = 0.5\n",
    "    This result means that there is a 50% dissimilarity between the string values of the two JSON objects.'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    path1 = input(\"Enter the file path for JSON 1: \").strip('\"')\n",
    "    path2 = input(\"Enter the file path for JSON 2: \").strip('\"')\n",
    "\n",
    "    data1 = load_json(path1)\n",
    "    if data1 is None:\n",
    "        return\n",
    "\n",
    "    data2 = load_json(path2)\n",
    "    if data2 is None:\n",
    "        return\n",
    "\n",
    "    texts1, ids1, json_objects1 = json_to_text(data1)\n",
    "    texts2, ids2, json_objects2 = json_to_text(data2)\n",
    "\n",
    "    json_objects1 = preprocess_json_objects(json_objects1)\n",
    "    json_objects2 = preprocess_json_objects(json_objects2)\n",
    "\n",
    "    vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'))\n",
    "    tfidf1 = vectorizer.fit_transform(texts1)\n",
    "    tfidf2 = vectorizer.transform(texts2)\n",
    "\n",
    "    similarity_matrix = cosine_similarity(tfidf1, tfidf2)\n",
    "\n",
    "    # Plot threshold analysis to determine the optimal threshold\n",
    "    plot_threshold_analysis(similarity_matrix)\n",
    "\n",
    "    # Recommend the optimal threshold value based on aggregated metrics\n",
    "    recommend_optimal_threshold(similarity_matrix)\n",
    "\n",
    "    # Choose a threshold value based on the analysis plots\n",
    "    threshold = float(input(\"Enter the chosen threshold value: \"))\n",
    "\n",
    "    # Build separate graphs for each JSON dataset\n",
    "    G1 = build_graph(json_objects1, '_blue', 'blue')\n",
    "    G2 = build_graph(json_objects2, '_red', 'red')\n",
    "\n",
    "    visualize_graph(G1)\n",
    "    visualize_graph(G2)\n",
    "\n",
    "    # Calculate and print distances\n",
    "    calculate_distances(json_objects1, json_objects2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
