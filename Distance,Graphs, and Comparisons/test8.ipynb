{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import silhouette_score\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "\n",
    "# Load the spaCy NLP model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Ensure necessary resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def load_json(file_path):\n",
    "    \"\"\"Load JSON data from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found - {file_path}\")\n",
    "        return None\n",
    "\n",
    "def json_to_text(data):\n",
    "    \"\"\"Convert JSON entries to text for analysis.\"\"\"\n",
    "    texts = []\n",
    "    ids = []\n",
    "    json_objects = {}\n",
    "    for entry in data:\n",
    "        text = \". \".join(f\"{key}: {value}\" if not isinstance(value, list) else f\"{key}: \" + \", \".join(map(str, value)) for key, value in entry.items())\n",
    "        texts.append(text)\n",
    "        ids.append(entry['id'])\n",
    "        json_objects[entry['id']] = entry\n",
    "    return texts, ids, json_objects\n",
    "\n",
    "def preprocess_json_objects(json_objects):\n",
    "    \"\"\"Ensure all JSON objects have required fields.\"\"\"\n",
    "    for obj in json_objects.values():\n",
    "        obj.setdefault('name', 'Unknown Name')\n",
    "        obj.setdefault('type', 'Unknown Type')\n",
    "        obj.setdefault('description', 'No description provided')\n",
    "    return json_objects\n",
    "\n",
    "def extract_verb_centered_snippet(description, window=2, max_gap=4):\n",
    "    \"\"\"Extract snippets around the first two verbs found in the description.\"\"\"\n",
    "    doc = nlp(description)\n",
    "    verbs = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'VERB':\n",
    "            verbs.append((token.i, token))\n",
    "            if len(verbs) == 2:\n",
    "                break\n",
    "\n",
    "    if not verbs:\n",
    "        return description\n",
    "\n",
    "    if len(verbs) == 1:\n",
    "        start = max(0, verbs[0][1].i - window)\n",
    "        end = min(len(doc), verbs[0][1].i + window + 1)\n",
    "        return doc[start:end].text\n",
    "\n",
    "    v1, v2 = verbs\n",
    "    if (v2[0] - v1[0]) <= max_gap:\n",
    "        start = max(0, v1[1].i - window)\n",
    "        end = min(len(doc), v2[1].i + window + 1)\n",
    "        return doc[start:end].text\n",
    "    else:\n",
    "        snippet1 = doc[max(0, v1[1].i - window):v1[1].i + window + 1].text\n",
    "        snippet2 = doc[max(0, v2[1].i - window):v2[1].i + window + 1].text\n",
    "        return f\"{snippet1} ... {snippet2}\"\n",
    "\n",
    "def build_graph(json_objects1, json_objects2, threshold, similarity_matrix, ids1, ids2):\n",
    "    \"\"\"Build a graph based on JSON objects and similarity matrix.\"\"\"\n",
    "    G = nx.Graph()\n",
    "    valid_nodes = set()\n",
    "\n",
    "    # Add nodes from both JSON datasets, ensuring unique IDs\n",
    "    for json_objects, dataset_suffix, dataset_color in [(json_objects1, '_blue', 'blue'), (json_objects2, '_red', 'red')]:\n",
    "        for key, obj in json_objects.items():\n",
    "            if obj['type'] != 'relationship':  # Only add entities as nodes\n",
    "                if obj.get('name') != 'Unknown Name':  # Avoid adding unknown name nodes\n",
    "                    unique_id = key + dataset_suffix\n",
    "                    node_name = f\"{obj.get('name', 'Unknown Entity')} ({dataset_color})\"\n",
    "                    G.add_node(unique_id, label=node_name, color=dataset_color,\n",
    "                               description=obj.get('description', 'No description provided'))\n",
    "                    valid_nodes.add(unique_id)\n",
    "                    print(f\"Added node: {unique_id} with name: {node_name}\")\n",
    "                else:\n",
    "                    print(f\"Filtered out node with id: {key} due to 'Unknown Name'\")\n",
    "\n",
    "    # Add edges from similarity matrix\n",
    "    for i, row in enumerate(similarity_matrix):\n",
    "        for j, similarity in enumerate(row):\n",
    "            if similarity > threshold:\n",
    "                source = ids1[i] + \"_blue\"\n",
    "                target = ids2[j] + \"_red\"\n",
    "                if source in valid_nodes and target in valid_nodes:\n",
    "                    G.add_edge(source, target, weight=similarity, style='solid', color='black', label='similar-to')\n",
    "                    print(f\"Added edge: {source} -> {target} with similarity {similarity}\")\n",
    "\n",
    "    return G\n",
    "\n",
    "def get_optimal_figsize(num_nodes):\n",
    "    \"\"\"Determine an optimal figure size based on the number of nodes.\"\"\"\n",
    "    base_size = 10  # Base size for the figure\n",
    "    scale_factor = 0.5  # Scale factor to adjust size\n",
    "    return (base_size + num_nodes * scale_factor, base_size + num_nodes * scale_factor)\n",
    "\n",
    "def get_optimal_k(num_nodes):\n",
    "    \"\"\"Determine an optimal k value for spring_layout based on the number of nodes.\"\"\"\n",
    "    base_k = 0.5  # Base k value\n",
    "    scale_factor = 0.1  # Scale factor to adjust k\n",
    "    return base_k + num_nodes * scale_factor\n",
    "\n",
    "def visualize_similarity_graph(G, unmatched_nodes):\n",
    "    \"\"\"Visualize a networkx graph with enhanced label formatting for readability.\"\"\"\n",
    "    num_nodes = len(G.nodes)\n",
    "    figsize = get_optimal_figsize(num_nodes)\n",
    "    k = get_optimal_k(num_nodes)\n",
    "\n",
    "    plt.figure(figsize=figsize)  # Dynamically set figure size\n",
    "    pos = nx.spring_layout(G, k=k)  # Dynamically set layout parameter\n",
    "\n",
    "    # Draw nodes with their corresponding colors\n",
    "    node_colors = [data.get('color', 'red' if node in unmatched_nodes else 'grey') for node, data in G.nodes(data=True)]\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=300)\n",
    "\n",
    "    # Draw edges with specified styles and colors\n",
    "    for (u, v, data) in G.edges(data=True):\n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos, edgelist=[(u, v)],\n",
    "            style=data.get('style', 'dotted' if data['color'] == 'gray' else 'solid'),\n",
    "            edge_color=data.get('color', 'black'),\n",
    "            width=2\n",
    "        )\n",
    "\n",
    "    # Draw edge labels\n",
    "    edge_labels = {(u, v): data.get('label', 'similar-to') for u, v, data in G.edges(data=True)}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)\n",
    "\n",
    "    # Draw labels for nodes\n",
    "    labels = {}\n",
    "    for node, data in G.nodes(data=True):\n",
    "        label = f\"{data.get('label', 'Unknown')}\\n({data.get('type', 'Unknown')})\"\n",
    "        description = data.get('description', 'No description provided')\n",
    "        if description:\n",
    "            snippet = extract_verb_centered_snippet(description)\n",
    "            label += f\"\\n{snippet}\"\n",
    "        labels[node] = label\n",
    "\n",
    "    for node, label in labels.items():\n",
    "        x, y = pos[node]\n",
    "        plt.text(x, y, label, fontsize=9, ha='center', va='center',\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.5\", facecolor='white', edgecolor='gray', alpha=0.6))\n",
    "\n",
    "    plt.title('Graph Visualization')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Print unmatched nodes\n",
    "    print(\"Unmatched Nodes:\")\n",
    "    for node in unmatched_nodes:\n",
    "        print(f\"{node}: {G.nodes[node]['label']}\")\n",
    "\n",
    "def plot_threshold_analysis(similarity_matrix):\n",
    "    thresholds = np.linspace(0, 1, 50)\n",
    "    num_edges = []\n",
    "    silhouette_scores = []\n",
    "    num_components = []\n",
    "    avg_clustering = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        G = nx.Graph()\n",
    "        labels = np.zeros(len(similarity_matrix))  # Correct the size of the labels array\n",
    "        label_idx = 1\n",
    "        for i, row in enumerate(similarity_matrix):\n",
    "            for j, similarity in enumerate(row):\n",
    "                if similarity > threshold:\n",
    "                    G.add_edge(i, j)\n",
    "                    if labels[i] == 0:\n",
    "                        labels[i] = label_idx\n",
    "                        label_idx += 1\n",
    "                    if j < len(labels):\n",
    "                        labels[j] = labels[i]\n",
    "        num_edges.append(len(G.edges))\n",
    "        num_components.append(nx.number_connected_components(G))\n",
    "        if len(G) > 0:\n",
    "            avg_clustering.append(nx.average_clustering(G))\n",
    "        else:\n",
    "            avg_clustering.append(0)\n",
    "        if len(set(labels)) > 1:\n",
    "            silhouette_scores.append(silhouette_score(similarity_matrix, labels))\n",
    "        else:\n",
    "            silhouette_scores.append(-1)\n",
    "\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.plot(thresholds, num_edges, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Number of Edges (Find the Elbow Point)')\n",
    "    plt.title('Elbow Method for Optimal Threshold')\n",
    "\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.plot(thresholds, silhouette_scores, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Silhouette Score (Higher is Better)')\n",
    "    plt.title('Silhouette Analysis for Optimal Threshold')\n",
    "\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.plot(thresholds, num_components, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Number of Components (Stable Plateau)')\n",
    "    plt.title('Number of Connected Components')\n",
    "\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.plot(thresholds, avg_clustering, marker='o')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Average Clustering Coefficient (Higher is Better)')\n",
    "    plt.title('Average Clustering Coefficient')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def calculate_similarity_matrix(texts1, texts2):\n",
    "    \"\"\"Calculate the cosine similarity matrix for two sets of texts.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'))\n",
    "    tfidf1 = vectorizer.fit_transform(texts1)\n",
    "    tfidf2 = vectorizer.transform(texts2)\n",
    "    return cosine_similarity(tfidf1, tfidf2)\n",
    "\n",
    "def recommend_optimal_threshold(similarity_matrix):\n",
    "    thresholds = np.linspace(0, 1, 50)\n",
    "    aggregated_scores = []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        G = nx.Graph()\n",
    "        labels = np.zeros(len(similarity_matrix))\n",
    "        label_idx = 1\n",
    "        for i, row in enumerate(similarity_matrix):\n",
    "            for j, similarity in enumerate(row):\n",
    "                if similarity > threshold:\n",
    "                    G.add_edge(i, j)\n",
    "                    if labels[i] == 0:\n",
    "                        labels[i] = label_idx\n",
    "                        label_idx += 1\n",
    "                    if j < len(labels):\n",
    "                        labels[j] = labels[i]\n",
    "        num_edges = len(G.edges)\n",
    "        num_components = nx.number_connected_components(G)\n",
    "        avg_clustering = nx.average_clustering(G) if len(G) > 0 else 0\n",
    "        silhouette = silhouette_score(similarity_matrix, labels) if len(set(labels)) > 1 else -1\n",
    "\n",
    "        aggregated_score = silhouette * avg_clustering / num_components if num_components > 0 else 0\n",
    "        aggregated_scores.append(aggregated_score)\n",
    "\n",
    "    optimal_threshold_idx = np.argmax(aggregated_scores)\n",
    "    optimal_threshold = thresholds[optimal_threshold_idx]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(thresholds, aggregated_scores, marker='o')\n",
    "    plt.axvline(optimal_threshold, color='red', linestyle='--')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Aggregated Score')\n",
    "    plt.title('Recommended Threshold Analysis')\n",
    "    plt.show()\n",
    "\n",
    "    return optimal_threshold\n",
    "\n",
    "def main():\n",
    "    path1 = input(\"Enter the file path for JSON 1: \").strip('\"')\n",
    "    path2 = input(\"Enter the file path for JSON 2: \").strip('\"')\n",
    "\n",
    "    data1 = load_json(path1)\n",
    "    if data1 is None:\n",
    "        return\n",
    "\n",
    "    data2 = load_json(path2)\n",
    "    if data2 is None:\n",
    "        return\n",
    "\n",
    "    texts1, ids1, json_objects1 = json_to_text(data1)\n",
    "    texts2, ids2, json_objects2 = json_to_text(data2)\n",
    "\n",
    "    json_objects1 = preprocess_json_objects(json_objects1)\n",
    "    json_objects2 = preprocess_json_objects(json_objects2)\n",
    "\n",
    "    similarity_matrix = calculate_similarity_matrix(texts1, texts2)\n",
    "\n",
    "    # Plot threshold analysis to determine the optimal threshold\n",
    "    plot_threshold_analysis(similarity_matrix)\n",
    "\n",
    "    # Choose a threshold value based on the analysis plots\n",
    "    threshold = recommend_optimal_threshold(similarity_matrix)\n",
    "    print(f\"Recommended Threshold: {threshold}\")\n",
    "\n",
    "    G = build_graph(json_objects1, json_objects2, threshold, similarity_matrix, ids1, ids2)\n",
    "    unmatched_nodes = []\n",
    "\n",
    "    for id1 in ids1:\n",
    "        if not any(similarity_matrix[ids1.index(id1)][j] > threshold for j in range(len(ids2))):\n",
    "            unmatched_nodes.append(id1 + \"_blue\")\n",
    "            if id1 + \"_blue\" not in G.nodes and json_objects1[id1]['name'] != 'Unknown Name':\n",
    "                G.add_node(id1 + \"_blue\", label=json_objects1[id1]['name'], type=json_objects1[id1]['type'], description=json_objects1[id1]['description'])\n",
    "\n",
    "    for id2 in ids2:\n",
    "        if not any(similarity_matrix[i][ids2.index(id2)] > threshold for i in range(len(ids1))):\n",
    "            unmatched_nodes.append(id2 + \"_red\")\n",
    "            if id2 + \"_red\" not in G.nodes and json_objects2[id2]['name'] != 'Unknown Name':\n",
    "                G.add_node(id2 + \"_red\", label=json_objects2[id2]['name'], type=json_objects2[id2]['type'], description=json_objects2[id2]['description'])\n",
    "\n",
    "    relationship_nodes = [node for node in G.nodes if 'relationship' in node]\n",
    "    G.remove_nodes_from(relationship_nodes)\n",
    "\n",
    "    visualize_similarity_graph(G, unmatched_nodes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
